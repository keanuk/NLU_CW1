Retained 2000 words from 9954 (88.35% of all tokens)


Training model for 10 epochs
training set: 25000 sentences (batch size 100)
Optimizing loss on 1000 sentences
Vocab size: 2000
Hidden units: 200
Steps for back propagation: 5
Initial learning rate set to 2.0, annealing set to 5

calculating initial mean loss on dev set: 11.457578352515567
calculating initial acc on dev set: 0.0

epoch 1, learning rate 2.0000	instance 1	epoch done in 1050.07 seconds	new loss: 0.5484689011732271	new acc: 0.712
epoch 2, learning rate 1.6667	instance 1	epoch done in 923.55 seconds	new loss: 0.5201138603956005	new acc: 0.731
epoch 3, learning rate 1.4286	instance 1	epoch done in 841.09 seconds	new loss: 0.5230300358649506	new acc: 0.75
epoch 4, learning rate 1.2500	instance 1	epoch done in 862.21 seconds	new loss: 0.5456282290747101	new acc: 0.687
epoch 5, learning rate 1.1111	instance 1	epoch done in 847.31 seconds	new loss: 0.39858134884064406	new acc: 0.832
epoch 6, learning rate 1.0000	instance 1