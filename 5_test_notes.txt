Format: python rnn.py train-np ~/Documents/NLU/Courseworks/NLU_CW1/data hdim lookback lr vocab_size epochs anneal batch_size

*Best loss and best loss accuracy based on dev set
**Accuracy based on performance on full test set 

Constants:
    train_size = 2000
    dev_size = 1000
    test_size = FULL
    lookback = 5
    epochs = 10
    python rnn.py train-np ~/Documents/NLU/Courseworks/NLU_CW1/data hdim 5 lr vocab_size 10 anneal batch_size

Variable:
    vocab_size
    hdim
    lr
    anneal
    batch_size

Run 1: Lower values
    Base values:
        vocab_size = 2000
        hdim = 50
        lr = 0.5
        anneal = 5
        batch_size = 100
        python rnn.py train-np ~/Documents/NLU/Courseworks/NLU_CW1/data 50 5 0.5 2000 10 5 100
        *Best loss: 0.6019136182193647
        *Best loss accuracy: 0.674
        **Accuracy: 0.686
    Alterations:
        +vocab_size = 1000
            *Best loss: 0.5821560186314653 (epoch 10)
            *Best loss accuracy: 0.68
            **Accuracy: 0.692
        +hdim = 15
            *Best loss: 0.6136658489596325 (epoch 10)
            *Best loss accuracy: 0.684
            **Accuracy: 0.694
        =lr = 0.1
            *Best loss: 0.6499571908199566 (epoch 10)
            *Best loss accuracy: 0.668
            **Accuracy: 0.683
        =anneal = 1
            *Best loss: 0.6262350393476706 (epoch 10)
            *Best loss accuracy: 0.669
            **Accuracy: 0.683
        +batch_size = 50
            *Best loss: 0.5555146037570908 (epoch 10)
            *Best loss accuracy: 0.729
            **Accuracy: 0.716
      
Run 2: Higher values
    Base values:
        vocab_size = 2000
        hdim = 50
        lr = 0.5
        anneal = 5
        batch_size = 100
        python rnn.py train-np ~/Documents/NLU/Courseworks/NLU_CW1/data 50 5 0.5 2000 10 5 100
        *Best loss: 0.6019136182193647
        *Best loss accuracy: 0.674
        **Accuracy: 0.686
    Alterations:
        =vocab_size = 4000
            *Best loss: 0.6297816158997828, (epoch 9)
            *Best loss accuracy: 0.669
            **Accuracy: 0.683
        +hdim = 100
            *Best loss: 0.5679797926112033 (epoch 10)
            *Best loss accuracy: 0.724
            **Accuracy: 0.709
        +lr = 1
            *Best loss: 0.5728468293490644 (epoch 10)
            *Best loss accuracy: 0.705
            **Accuracy: 0.712
        +anneal = 10
            *Best loss: 0.5931426739795183 (epoch 10)
            *Best loss accuracy: 0.691
            **Accuracy: 0.696
        -batch_size = 200
            *Best loss: 0.6299458398336376, (epoch 10)
            *Best loss accuracy: 0.665
            **Accuracy: 0.682
