Retained 2000 words from 9954 (88.35% of all tokens)


Training model for 10 epochs
training set: 1000 sentences (batch size 100)
Optimizing loss on 1000 sentences
Vocab size: 2000
Hidden units: 25
Steps for back propagation: 0
Initial learning rate set to 0.5, annealing set to 5

calculating initial mean loss on dev set: 7.798662515757492

epoch 1, learning rate 0.5000	instance 1	epoch done in 37.20 seconds	new loss: 8.150146536964161
epoch 2, learning rate 0.4167	instance 1	epoch done in 36.50 seconds	new loss: 5.978210046559437
epoch 3, learning rate 0.3571	instance 1	epoch done in 36.82 seconds	new loss: 5.746970466401882
epoch 4, learning rate 0.3125	instance 1	epoch done in 36.32 seconds	new loss: 5.19594802314579
epoch 5, learning rate 0.2778	instance 1	epoch done in 36.43 seconds	new loss: 5.1541201294857775
epoch 6, learning rate 0.2500	instance 1	epoch done in 36.49 seconds	new loss: 5.115809217899049
epoch 7, learning rate 0.2273	instance 1	epoch done in 36.40 seconds	new loss: 5.114597758027079
epoch 8, learning rate 0.2083	instance 1	epoch done in 37.17 seconds	new loss: 5.046921756071073
epoch 9, learning rate 0.1923	instance 1	epoch done in 36.28 seconds	new loss: 5.029777551548091
epoch 10, learning rate 0.1786	instance 1	epoch done in 35.97 seconds	new loss: 5.0169758618375

training finished after reaching maximum of 10 epochs
best observed loss was 5.0169758618375, at epoch 10
setting U, V, W to matrices from best epoch

==========================
Matrix U:
 [[-1.74515943e+00 -2.33429649e-01 -1.52956027e+00 -2.06775862e+00
  -2.22385387e-01 -1.99116055e+00 -3.66906477e-01 -4.31388985e-01
  -8.21558328e-01 -1.49258801e+00 -1.94551484e+00 -4.10360355e-01
  -8.47527591e-01 -2.12007633e+00 -6.16094846e-02 -1.29771878e-01
  -8.31260480e-01 -1.43098240e+00 -8.85882249e-01 -2.64868967e+00
  -3.66899174e-01 -5.20658351e-02 -7.99346224e-01 -1.23927760e+00
  -1.11677029e+00]
 [-1.81313901e-01 -5.69555264e-01 -9.72835100e-01 -1.34599436e+00
  -5.60813583e-01 -3.54149525e-01 -3.21991051e-01 -7.29518575e-01
  -7.84023044e-01 -1.50351189e+00 -9.21134061e-02 -6.52801695e-01
  -4.70284144e-01 -6.30732085e-01 -3.09909796e-01 -7.94028454e-01
  -1.04068780e+00 -9.12247528e-01 -8.77797516e-01 -5.95903127e-01
  -7.41625854e-01 -1.21634124e+00 -7.26185758e-02 -3.66989624e-01
   5.35871657e-02]
 [-6.77214567e-01 -5.36450758e-01 -1.26621504e+00 -1.16108113e+00
  -3.15505107e-01 -8.43023077e-01  4.07006618e-01 -1.22392884e-01
  -5.97059576e-02 -1.62786937e+00 -1.00424863e+00  3.72266142e-01
  -1.18019756e-01 -8.01790990e-01 -4.25169662e-01  9.99168214e-03
  -1.10227714e-01 -6.87872927e-01  1.74025834e-02 -1.32854547e+00
  -3.35821965e-01 -2.74568933e-01  5.83944218e-01  2.12854776e-01
  -1.65561721e-02]
 [-1.14592707e+00 -2.69527946e-01 -2.30922036e+00 -1.23208634e+00
  -3.39933289e-01 -1.37813019e+00  3.52369115e-02  3.57093651e-01
  -3.43309232e-01 -2.17432784e+00 -8.80147997e-01  1.90466356e-01
   5.18744870e-01 -1.10079616e+00 -5.72183190e-01 -3.01729203e-01
   4.80980206e-02 -7.84177676e-01 -2.20826368e-01 -2.12272146e+00
  -1.93322065e-01 -6.31337280e-01 -2.87373037e-01 -2.43092156e-01
  -4.74459866e-01]
 [-6.97343664e-02 -1.05816785e+00  4.67834974e-02 -7.62054237e-01
  -9.63299104e-01 -8.11792298e-02 -3.62166392e-01 -2.28807895e+00
  -1.49369486e+00 -1.91293315e+00 -4.68737723e-01 -1.03331974e+00
  -1.19175652e+00 -8.76637691e-01 -8.42332972e-01 -1.78535710e+00
  -1.38083025e+00 -9.39858578e-01 -1.58070180e+00 -1.22232978e+00
  -9.47003427e-01 -1.15907271e+00 -4.69528030e-01 -1.08611998e+00
  -5.19893668e-01]
 [-5.98584402e-01 -2.11197429e-01 -7.33535373e-01 -1.13443407e-01
  -1.49444171e-01 -1.07147846e+00 -6.45205611e-01 -2.34137557e-01
  -8.85386685e-01 -1.64281953e+00 -1.03271238e+00  2.27237026e-02
  -4.12127840e-01 -1.01065420e+00 -5.12770625e-01 -6.60616821e-01
  -2.41908631e-02 -1.21668368e+00 -5.84785238e-01 -1.83834734e+00
  -1.20141672e-02 -2.78113260e-01 -3.67334604e-01 -2.68582455e-01
  -1.41155828e+00]
 [ 1.98736626e-01  8.43941373e-02 -6.17194629e-02 -7.60111756e-02
  -1.87322842e-01  1.11843199e-01  1.25524021e+00 -5.76439268e-01
  -1.21657570e+00  1.64935025e+00 -1.46567565e-01 -5.85837660e-02
  -2.09974645e-01 -1.37315026e-01 -7.77972515e-01 -1.25292552e+00
  -5.64485569e-01 -4.25757037e-01  5.08832992e-02  4.11645670e-01
  -4.78710831e-01  2.71831934e-02  5.11738892e-01  4.95419169e-01
   4.53951031e-01]
 [-1.02529730e-01 -1.15092926e+00 -1.01279372e+00 -1.50845947e+00
  -1.05185134e+00 -6.25144778e-01 -6.40526205e-01 -1.14260280e+00
  -1.32744092e+00 -1.90164428e+00 -5.25535807e-01 -1.21294903e+00
  -8.53239475e-01 -7.15647875e-01 -7.77511635e-01 -6.05316142e-01
  -1.00193109e+00 -5.66558900e-01 -1.40528776e+00 -9.33784447e-01
  -6.76425597e-01 -4.80987800e-01 -7.45964107e-01 -8.82250533e-01
  -1.99282741e-01]
 [-5.00391974e-01 -4.44988204e-01 -7.68777689e-01 -1.14100653e+00
  -1.33049810e-01 -5.83392699e-01  2.27013033e-01 -4.33529424e-01
  -1.35216682e+00 -1.49846665e+00 -5.24602240e-01 -4.30648098e-01
   1.71287979e-01 -1.27678755e+00 -1.48914436e+00 -2.11840508e-01
  -7.75888918e-01 -8.79434156e-01 -2.41212685e-01 -6.09428037e-01
  -1.37241209e+00 -9.59096290e-02  5.50888976e-01 -2.40201265e-02
  -2.46725562e-01]
 [ 1.29009067e+00 -3.71670177e-02  1.46567192e+00  1.51657667e+00
  -4.88835878e-01  2.18219394e+00  2.26096915e-01 -4.38020208e-01
   1.56616313e-01  1.60363337e+00  1.76209910e+00 -2.21639263e-01
  -1.39293074e-01  1.61843707e+00 -1.17993406e-01 -5.71346840e-01
  -1.13078007e-01  3.14612542e-01 -5.55999474e-01  1.58458813e+00
  -4.86997363e-01  5.20235597e-01  9.68100281e-01  1.32120048e+00
   9.74284795e-01]
 [-8.62278006e-01 -4.11787356e-01 -1.75307825e+00 -1.44510641e+00
   2.54127119e-01 -9.41513701e-01 -4.96967032e-02  1.97089755e-01
  -7.73466934e-01 -2.17601756e+00 -1.96620104e+00 -4.94078953e-01
   9.39456727e-02 -1.66533552e+00 -3.55491509e-01 -3.25224239e-01
   1.71861824e-01 -1.17134093e+00 -2.37741172e-01 -2.01686503e+00
  -6.30319163e-01 -5.39560285e-01 -5.14912395e-01 -4.97723134e-01
  -8.58363961e-01]
 [-1.08025925e-01 -1.01155834e+00 -8.70615741e-01 -1.19251645e+00
  -5.88561824e-01 -7.35505666e-01 -2.49811623e-01 -1.51194180e+00
  -1.58485124e+00 -1.89935047e+00 -4.95792147e-03 -1.05393705e+00
  -8.30931258e-01 -6.52336271e-01 -7.53137631e-01 -1.53557489e+00
  -1.15128934e+00 -1.50202098e+00 -1.18592224e+00 -1.01625201e+00
  -1.35901068e+00 -1.13549676e+00 -1.06917671e+00 -6.07705926e-01
  -1.87411079e-01]
 [-1.46898972e-01 -7.14782097e-01 -6.57913445e-01 -6.45594827e-01
  -1.44337361e+00 -3.26349332e-01 -7.24239703e-01 -1.91480154e+00
  -1.77070056e+00 -1.30472385e+00 -5.34365352e-01 -7.11373301e-01
  -1.01645716e+00 -6.75096991e-01 -1.48226250e+00 -1.52592809e+00
  -1.16015685e+00 -1.66702427e-01 -1.57891077e+00 -7.94954614e-01
  -1.23797921e+00 -5.08351085e-01 -5.88177259e-01 -1.30175760e+00
  -1.13315401e+00]
 [ 5.81184191e-03 -5.02107621e-01 -6.96368332e-01 -9.63694075e-01
   3.00387772e-01 -2.79856947e-01 -2.76405911e-01 -7.52900701e-01
  -8.19799678e-01 -2.17914773e+00 -1.67005151e-01 -4.91159060e-01
  -3.34092693e-01 -9.23499694e-01 -4.36885173e-01 -7.72998581e-01
  -4.39487288e-01 -4.96057755e-02 -5.93855728e-01 -1.66499739e+00
  -5.32265058e-01  3.56229544e-01  2.36068481e-01 -4.01299245e-01
   2.01399942e-01]
 [-6.05530745e-01 -9.52212247e-01 -5.94019652e-01 -5.94253750e-01
  -1.58201707e+00  2.12302725e-04 -5.31920680e-01 -1.24944776e+00
  -1.57761884e+00 -1.41805147e+00  3.19427688e-01 -1.61625052e+00
  -8.36061478e-01 -1.56854237e-01 -1.41444169e+00 -1.57882854e+00
  -1.41618988e+00 -8.69506516e-01 -1.50540200e+00 -5.47475873e-01
  -1.26713801e+00 -1.06107785e+00 -4.19743930e-01 -4.24288733e-01
  -2.51195543e-01]
 [-4.56739917e-02 -5.26079995e-01 -7.14824824e-01 -1.50971750e+00
  -3.79590943e-01 -2.87099520e-01 -2.00939326e-01 -9.68257583e-01
  -1.47933705e+00 -1.96826887e+00 -1.15568437e+00 -9.01337603e-01
   3.79702693e-02 -5.35388716e-01 -1.04013195e+00 -1.05939317e+00
  -1.08194509e+00 -6.39494430e-01 -1.20523105e+00 -1.15421998e+00
  -8.39894072e-01 -4.74370793e-01 -1.57090852e-01 -2.92669749e-01
  -2.35994586e-01]
 [ 1.54371208e-01 -1.12953792e+00 -2.39539397e-01 -1.22291310e+00
  -1.28304056e+00 -9.60595079e-02 -5.35131241e-01 -1.62805262e+00
  -1.89862024e+00 -1.51917583e+00  1.72413989e-01 -1.77237809e+00
  -1.15682766e+00 -9.53677381e-01 -1.35082815e+00 -1.92031924e+00
  -1.51813306e+00 -7.01667617e-01 -1.53850937e+00 -6.01688517e-01
  -1.56704445e+00 -1.39682083e+00 -7.35976407e-01 -6.68262939e-01
  -9.85639851e-01]
 [-1.62392479e+00 -2.43809991e-01 -1.53262947e+00 -1.53735042e+00
   8.51444404e-02 -2.25554823e+00  8.84952773e-01 -6.39871334e-02
  -1.99828053e-01 -5.49863686e-01 -1.62712083e+00  2.58326781e-01
   1.71918239e-01 -2.23975243e+00  3.12687047e-01 -8.19469662e-02
  -4.09633482e-01 -1.09618170e+00 -3.85066174e-01 -2.27096749e+00
   7.03969833e-01 -1.41884541e-01  2.99697321e-01 -4.02447952e-02
  -5.75733486e-01]
 [-2.95086762e-02 -1.04098016e+00 -5.08717533e-01  5.97473530e-03
  -4.91116565e-01 -1.27682324e-01 -1.31229737e-01 -1.43922988e+00
  -1.79031087e+00 -2.15128363e+00 -5.49279656e-01 -9.61463706e-01
  -5.30599352e-01 -5.64724785e-01 -9.17887695e-01 -1.47674670e+00
  -9.85573832e-01 -1.18953113e-01 -1.23703950e+00 -1.00617205e+00
  -1.03553455e+00 -5.48860082e-01 -7.38550520e-01 -1.62183976e-01
  -2.00872500e-01]
 [-2.59842138e-01 -1.39144413e-01 -5.05578771e-01 -4.67595482e-02
  -3.80348354e-01 -4.06609190e-01  9.02916290e-01  4.47020225e-02
  -9.50404907e-02 -3.13126182e-01  2.02392709e-02 -2.65271593e-01
   3.77093401e-01 -3.18200277e-01 -4.80546414e-03  2.65392042e-01
  -3.58758175e-01 -4.34621349e-02 -2.41754213e-01 -5.12966425e-01
  -1.71464477e-01  6.72529682e-02  1.18697303e+00  1.07032527e-01
   9.92227918e-01]
 [-2.20275759e-01 -1.33893985e+00 -9.11685229e-01 -8.46765276e-01
  -1.74486066e+00 -1.26907440e-01 -5.16991141e-01 -2.58635243e+00
  -2.58437966e+00 -2.30341410e+00 -3.76543282e-01 -1.50785739e+00
  -1.09079910e+00 -8.27830230e-01 -1.05124153e+00 -2.06718375e+00
  -1.76492494e+00 -1.32969534e+00 -2.24739846e+00 -6.10085416e-01
  -2.03478631e+00 -1.67565782e+00 -9.83319480e-01 -5.96052585e-01
  -4.69141726e-01]
 [-1.34628924e-01 -1.21718327e+00  2.65618499e-01 -7.38655513e-01
  -1.02195610e+00  5.68285998e-01  7.65203066e-02 -9.40701595e-01
  -1.11225884e+00 -5.69664615e-01  1.51289630e-02 -1.42993004e+00
  -9.66814457e-01  3.76875523e-01 -9.04407551e-01 -1.39574904e+00
  -1.69418165e+00  6.02718750e-01 -1.01802691e+00  5.39719290e-01
  -1.14219052e+00 -5.35794711e-01 -8.43512907e-01 -2.74762362e-01
   9.58069536e-02]
 [ 3.83900540e-01 -4.12787871e-01  4.09106154e-01 -6.89249785e-02
  -3.08520742e-01  3.05147967e-01  4.18302103e-01 -8.05931750e-01
  -5.80307810e-01  1.00710436e+00  2.08234084e-01 -5.03216011e-01
  -1.98625990e-01 -1.37091199e-02 -4.96879464e-01 -8.95381514e-01
   3.19946572e-01  4.81338858e-02 -7.55257381e-01  1.24595404e-01
  -7.61790924e-01 -3.17581920e-01 -3.89405861e-01  4.14445782e-01
   4.68486397e-01]
 [ 8.73214377e-01 -5.88987018e-01  7.93915234e-01  3.08561336e-01
  -6.15925484e-01  6.32769363e-01 -4.28741649e-01 -1.05605669e+00
  -6.85953075e-01 -2.50221322e-01  8.82276499e-01 -2.33980145e-01
  -2.26506787e-01  3.79018013e-01 -8.01655221e-01 -9.68304928e-01
  -3.62282865e-01  6.20460664e-01 -1.35679414e+00  8.24836055e-01
  -8.85601106e-01 -5.71829283e-01  6.72072942e-01 -2.09043254e-01
  -1.53329482e-01]
 [-3.43609138e-01 -4.15340600e-01 -3.26444371e-01 -1.88167813e-01
  -9.73145148e-01 -4.47787841e-01  2.91714320e-01 -7.79678628e-01
  -8.06140072e-01 -3.00600101e-01 -2.52721129e-01 -4.79369376e-01
  -5.75072882e-01 -5.82039009e-01 -4.17525443e-01 -6.58463869e-01
  -6.65143214e-01 -4.34658964e-01 -1.11647013e+00 -8.23554841e-01
  -3.08596168e-01  4.26452676e-01 -1.27399099e-01  9.65104004e-02
   2.75981013e-02]] 
Matrix V:
 [[ 2.42169479e-02 -1.26138149e-01 -5.39441146e-01 ...  2.33562692e-04
   9.37034991e-02  1.41230605e-01]
 [ 2.56913848e-01  2.07178111e-01 -9.96535677e-01 ... -2.99764879e-01
  -4.85932386e-01  6.67193507e-01]
 [-1.47372705e-01  2.26501296e-01 -4.10472829e-01 ...  3.59733418e-01
  -2.95929238e-01  1.09844786e-03]
 ...
 [-1.83566807e-01 -2.11251384e-01 -3.61583262e-01 ...  3.73086618e-02
  -5.77507922e-02 -2.43406046e-01]
 [-1.77992490e-01  1.73430828e-01 -1.13596227e+00 ...  3.22895578e-01
   1.82272407e-01  3.88607628e-01]
 [-1.92146945e-01 -6.87216514e-02 -7.32949477e-01 ...  2.69581906e-01
   1.97875164e-01  2.51320619e-01]] 
Matrix W:
 [[-0.08294037  0.34576608  0.37300821 ...  0.51015446  1.36545498
   0.38887981]
 [ 0.04222235  0.07478691 -0.12018967 ...  0.50991479  0.20604367
   0.29831029]
 [ 0.77442124  1.30126525  0.58027796 ...  0.40529342  1.36562414
   0.41232479]
 ...
 [-0.54596621  0.0856849  -0.45092935 ...  0.52770937  0.08821625
  -0.13203199]
 [-0.36523588 -0.07091562  0.23507526 ...  0.14767568  0.00541547
   0.24106927]
 [-0.32654506 -0.14939746 -0.31243484 ... -0.15775218 -0.0565044
  -0.17539478]] 
===========================


Training model for 10 epochs
training set: 1000 sentences (batch size 100)
Optimizing loss on 1000 sentences
Vocab size: 2000
Hidden units: 25
Steps for back propagation: 2
Initial learning rate set to 0.5, annealing set to 5

calculating initial mean loss on dev set: 8.122025716192606

epoch 1, learning rate 0.5000	instance 1	epoch done in 41.84 seconds	new loss: 7.697708658550072
epoch 2, learning rate 0.4167	instance 1	epoch done in 41.03 seconds	new loss: 5.874172013018105
epoch 3, learning rate 0.3571	instance 1	epoch done in 41.37 seconds	new loss: 5.381247748626463
epoch 4, learning rate 0.3125	instance 1	epoch done in 41.01 seconds	new loss: 5.1305880727230555
epoch 5, learning rate 0.2778	instance 1	epoch done in 40.97 seconds	new loss: 5.095481254671179
epoch 6, learning rate 0.2500	instance 1	epoch done in 41.33 seconds	new loss: 5.047728996453036
epoch 7, learning rate 0.2273	instance 1	epoch done in 41.02 seconds	new loss: 5.0204814488637295
epoch 8, learning rate 0.2083	instance 1	epoch done in 41.21 seconds	new loss: 5.004952059268629
epoch 9, learning rate 0.1923	instance 1	epoch done in 41.67 seconds	new loss: 4.994626440019564
epoch 10, learning rate 0.1786	instance 1	epoch done in 41.28 seconds	new loss: 4.978069232975868

training finished after reaching maximum of 10 epochs
best observed loss was 4.978069232975868, at epoch 10
setting U, V, W to matrices from best epoch

==========================
Matrix U:
 [[-2.18149872e+00 -3.35652888e-01 -4.95733255e-01 -7.66657696e-01
  -9.26838551e-01 -2.06915930e+00 -8.91125220e-01 -8.12176509e-01
  -3.66184319e-01 -1.45193285e+00 -1.06623256e+00 -1.46903983e-01
   8.21749329e-02 -1.64307297e+00  1.25688859e-01  1.81350994e-01
  -9.88134987e-01 -4.85572473e-01 -2.67145629e-01 -7.02604365e-01
  -1.60101014e+00 -1.96140421e-01 -1.34601109e-01  1.56537210e-01
   5.16013908e-02]
 [ 1.81430746e-01 -4.07226220e-01  5.49237909e-02 -2.36518037e-01
  -3.78093476e-01 -5.48511830e-01 -2.78490106e-01  2.27127176e-01
   1.62586587e-01  3.41930124e-01  1.56434424e-01 -3.43468167e-01
  -1.80283973e-01  1.48460024e-01 -9.54597984e-01 -2.96306573e-01
  -2.68999211e-01 -1.28965030e-01  4.62050538e-01 -9.65816934e-01
  -1.38057901e-02  5.49922722e-01 -3.63747971e-01 -4.64320959e-01
  -4.49235524e-01]
 [-7.81536492e-01 -1.51521538e-01 -6.51539855e-01 -1.73038607e-01
  -1.35458350e-01 -1.19087233e+00  3.47122120e-01  1.88159409e-01
  -2.78220643e-01 -8.12505016e-01 -6.19553661e-01 -2.08606513e-01
   1.73239814e-01 -6.43933558e-01 -1.83814253e-01 -5.11343886e-01
  -7.59974249e-01 -3.26542583e-01  9.76322074e-02 -4.46044968e-01
  -1.15048588e+00 -6.46031902e-01 -2.58799382e-01 -2.40705010e-01
  -4.25477988e-01]
 [-1.40723617e-01 -3.87030910e-01 -4.75725632e-01 -1.20830179e+00
  -4.56405795e-01 -1.29170608e+00 -6.12344242e-01 -1.86307983e-01
  -1.40384129e-01 -2.34976347e-03 -8.61118762e-01 -7.29068030e-01
  -4.79780712e-01  1.78366228e-01 -1.04026776e+00 -1.04985715e+00
   4.85275558e-01 -1.71281914e-01 -6.10778901e-01 -3.95311988e-01
  -1.05215347e-01 -4.81384311e-01 -4.26443086e-01 -1.10207919e+00
  -1.71577609e+00]
 [ 4.90314824e-01  2.02559822e-01 -6.51843042e-02 -1.71422313e-01
   1.78883981e+00  9.49747554e-01  8.52628253e-01  4.42053122e-01
   1.26943606e+00  1.96338561e-01  2.14778626e-01 -1.64129479e-01
   1.09298174e+00  1.87488796e-02  1.24156412e-01  5.96122266e-01
   3.30880770e-01 -7.33453901e-02  7.65011040e-01  4.84428861e-02
   3.76634332e-02  2.46249663e-01  1.03675050e+00  1.31582172e-01
  -2.89319273e-01]
 [-3.11255152e-01  6.83947914e-01  5.19016817e-01  7.37644713e-01
  -7.38232876e-01 -3.80952852e-01  2.62983512e-01  5.57796741e-01
   1.18707270e+00 -1.96237848e-01  2.24018732e-01  8.50803639e-01
   9.99350114e-01  1.26130580e-02  6.16701261e-01  1.00527782e+00
   2.69954530e-01  8.52763503e-01  7.11643370e-01  6.99720698e-01
  -1.03790332e-01  2.12767337e-01  3.23636156e-01  6.51267276e-01
   1.11672781e+00]
 [-5.51998620e-01 -6.60320536e-02 -2.64609032e-01  1.57720490e-01
   8.34534970e-01 -4.12054001e-01 -4.87366668e-01  8.55249274e-01
   4.75723843e-01 -5.92105485e-01 -4.37526718e-02 -2.25540138e-01
  -3.32212127e-01 -5.35275253e-01 -8.27234637e-01  1.18324609e-01
  -4.06419917e-01 -3.10254319e-01  1.16237127e-01  2.41997524e-01
  -4.14257132e-01 -6.10132535e-01  6.69285675e-02  1.04671148e-01
  -1.53327088e-01]
 [-3.87658294e-01 -4.37168842e-01 -3.98171455e-01 -3.45991427e-01
   1.89148363e-01 -4.32379545e-02  1.68968919e-01  6.54426659e-01
  -6.97671059e-01 -1.69384044e-01 -6.39351939e-01 -5.60576009e-01
   1.34574677e-01 -1.58904732e-01 -4.67144268e-01  9.06092861e-02
  -5.46369154e-01 -4.88133813e-01 -5.13357657e-01  8.09849028e-02
  -6.21317355e-01  3.02223160e-01  5.02397269e-01  2.82345403e-02
  -4.11556411e-01]
 [-2.00520944e-01 -3.97153719e-01 -4.64658891e-01 -9.06006979e-01
  -2.33591000e-01 -7.30855773e-01 -5.66493461e-01 -3.99615460e-01
  -2.03538728e-01  9.23874131e-02 -2.67971203e-01 -3.56556239e-01
   4.22465709e-02 -6.52963422e-01 -1.36997228e+00 -1.05086047e+00
  -4.67678741e-01  2.19753743e-01 -5.59798170e-01 -4.30019630e-01
  -9.05742117e-01 -4.11608833e-01 -5.66242168e-01 -6.03216917e-01
  -2.05229342e+00]
 [-2.30933997e+00 -5.63567179e-01 -1.16739957e+00 -5.58487759e-01
  -1.44290694e+00 -2.55725410e+00 -1.18737508e+00 -6.49513381e-01
   3.43115357e-01 -2.19265280e+00 -8.01546755e-01 -4.76830900e-01
  -1.22798099e+00 -1.50574968e+00 -1.02237654e+00 -8.03134924e-01
  -1.59071639e+00 -1.43327098e+00 -5.04149826e-01 -7.30531112e-01
  -2.29363899e+00  4.29914864e-04 -3.72938722e-01 -9.51739675e-01
  -6.29194368e-01]
 [-1.26438171e+00 -4.87111118e-01 -9.14041839e-01  2.34744979e-01
  -5.27248848e-01 -2.25792920e+00 -8.06447688e-01 -3.41130303e-01
   1.08158832e-01 -1.44451084e+00 -1.11621312e+00 -4.39154218e-01
  -2.71249920e-01 -1.51325722e+00 -7.15218566e-01 -3.17779587e-01
  -1.45781773e+00 -2.54235755e-01 -7.90785642e-01 -4.07510675e-01
  -2.03568290e+00 -4.69416870e-01 -1.00585316e+00 -2.12130126e-01
  -7.50983364e-02]
 [-5.53056721e-01 -6.70728974e-01  1.25194602e-01 -8.53473622e-01
   1.75464512e-02 -4.94454209e-01  2.27751734e-01  1.81983194e-01
   1.48950394e-01  7.91198247e-02 -3.08146146e-01  3.60376732e-01
   1.10841170e-01 -2.85419968e-02 -2.15624742e-01 -7.20439328e-01
  -1.63657495e-01 -8.35361463e-02 -3.32086056e-02 -8.32694052e-01
  -3.26114151e-01  3.17233893e-01  2.16374528e-01 -3.08503746e-01
  -4.53453817e-01]
 [-9.98666968e-01  1.72016497e-02 -8.73674007e-01  2.16003462e-01
  -2.54728848e-01 -6.26428326e-01 -2.65844722e-01 -1.78563922e-01
   1.44239351e-01 -5.75372599e-01 -7.45780570e-01  4.10861865e-02
  -1.01892486e-01 -7.94048390e-01 -9.01681709e-02 -3.96133481e-01
  -8.35320327e-01 -3.58698766e-01 -1.37764471e-01 -5.18044296e-01
  -5.62921371e-02 -3.30782539e-01 -4.51098212e-01 -1.76573489e-01
  -2.36496109e-01]
 [-2.55547129e+00 -5.81069077e-01 -8.60638722e-01 -4.52719329e-01
  -1.14759280e+00 -3.04616759e+00 -8.97010124e-01 -1.22240854e+00
   2.10085081e-02 -2.11772415e+00 -1.14948237e+00 -3.37934664e-01
  -5.27263404e-01 -2.23830660e+00 -8.43637052e-01 -7.64490980e-01
  -2.14101118e+00 -1.84735686e+00 -3.08092518e-01 -5.70237154e-01
  -2.82396650e+00 -2.39406635e-01 -5.31283372e-01  1.37495432e-03
  -5.73122682e-01]
 [ 2.82971882e-01 -4.82800442e-02 -2.43342686e-01 -9.86005289e-01
  -8.93114360e-01 -8.57798061e-01 -2.73550889e-01  3.84937551e-02
  -3.49881596e-03  4.45639391e-01 -5.45395880e-01 -4.47984928e-01
  -4.84661295e-01 -1.45880922e-01 -8.29267078e-01 -3.78269689e-01
  -1.05431009e-01 -5.76672497e-03 -2.78487726e-01 -7.25362645e-01
  -6.87580099e-01 -2.15251647e-01 -2.81317793e-01 -9.36302103e-01
  -7.16460019e-01]
 [ 1.68875767e-01 -4.84290963e-01  3.26569687e-01 -3.48821188e-01
   7.03140528e-02 -3.68231295e-01  1.24833966e-01  3.49681991e-01
  -1.62939349e-01  3.51041422e-01 -3.32187544e-01 -7.22088869e-01
  -4.02045126e-01  4.82503264e-01 -8.02336902e-01 -6.93439203e-01
   1.11800483e-01 -8.95736387e-02  2.86253862e-01 -2.95027201e-01
  -3.66508441e-01 -8.10960283e-01  1.50007543e-01  1.03491898e-01
  -6.87433725e-01]
 [-2.98045369e+00 -2.92686052e-01 -1.52389284e+00 -4.66213895e-01
  -1.35727253e+00 -3.65712576e+00 -9.06931602e-01 -5.80800471e-01
  -6.12748835e-01 -2.08540097e+00 -1.54307093e+00 -3.45916321e-01
  -1.13685470e+00 -1.90996570e+00 -6.22845046e-01 -8.18811239e-01
  -2.88099610e+00 -1.75681760e+00 -2.08104949e-01 -7.73197513e-01
  -2.79086195e+00 -1.00638202e+00 -8.60735539e-01 -2.65414732e-01
  -1.10949835e+00]
 [-1.03786061e+00 -5.35618847e-01 -2.34854703e-01 -6.32404692e-01
  -9.23713327e-01 -2.12538106e+00 -7.60195075e-01  1.25420862e-02
  -2.83797639e-01 -1.49092885e+00 -8.09789754e-01 -7.09505038e-01
  -4.65910194e-01 -9.78067668e-01 -7.14192370e-01 -8.62441242e-01
  -8.04711409e-01 -2.72440355e-01 -1.13135921e+00 -1.10628806e+00
  -1.46138785e+00 -6.24157563e-01 -2.28874908e-01 -2.87011630e-01
  -8.71137715e-01]
 [ 5.34110764e-01  4.77938928e-01 -2.85573215e-01 -6.01301431e-01
  -1.18540247e-01  1.28140938e-01  3.77200297e-01 -1.02323763e-01
  -8.74205734e-01  3.94481981e-01 -5.57524437e-02 -3.90427517e-01
  -2.09508145e-01  5.77389481e-01 -6.89376554e-01 -4.94790208e-01
   1.88205428e-01  3.44253590e-03 -4.02583822e-01 -4.11685155e-01
  -2.32804447e-01 -8.82654805e-01 -4.89650725e-01 -1.62700072e-01
  -9.82916769e-01]
 [-8.59866385e-01 -5.81827954e-01 -3.21074463e-01 -1.39225619e+00
  -7.24453953e-01 -2.59135439e+00 -7.85510783e-01 -4.07341636e-01
  -6.20726867e-01 -2.40592231e-01 -7.39710940e-01 -1.53703049e+00
  -3.05609562e-01 -5.64906748e-01 -2.01435512e+00 -2.09678462e+00
  -3.69951354e-01 -5.67294153e-01 -9.42343750e-01 -1.05075443e+00
  -1.50238653e+00 -1.56066046e+00 -1.23457584e+00 -2.29163069e-01
  -2.54792077e+00]
 [-1.70143192e+00  2.83175345e-01 -4.35229357e-01  3.44558064e-01
  -6.84663343e-01 -1.16310834e+00  3.90120112e-02  9.87736747e-02
  -4.64849292e-02 -2.81849586e-01 -2.80554570e-01  2.03609003e-01
  -3.35748607e-01 -7.53213548e-01 -3.61008847e-01  3.00886235e-01
  -2.17408942e-01 -9.79928581e-01 -3.11193724e-01 -4.71072165e-01
  -1.30913668e+00  4.44442983e-01 -6.63597034e-02  2.42056653e-01
  -7.13866180e-01]
 [-2.80698866e-01 -5.84385500e-01 -2.51355095e-01 -1.40122298e+00
  -1.11424977e+00 -1.58674589e+00 -8.85685611e-01 -6.33735008e-01
  -2.53632552e-01 -3.68951384e-01 -1.09224559e+00 -1.66812505e+00
  -7.79893437e-01 -8.94835540e-01 -1.56918067e+00 -1.95984359e+00
  -4.77255919e-01 -2.93928704e-01 -1.15451633e+00 -7.09828735e-01
  -1.23198711e+00 -1.13636503e+00 -2.61433792e-01 -7.38723300e-01
  -1.72840176e+00]
 [ 7.60369229e-01 -4.12387625e-02  1.86690435e-01 -2.41066569e-01
   7.51638061e-01 -5.40145998e-02 -2.22503749e-01 -8.98867651e-02
   1.64762604e-01 -2.32305332e-01  1.34106420e-01  9.39279902e-02
  -3.37331029e-01  2.58071642e-01 -2.87985002e-01 -6.03717786e-01
   4.81173028e-01  6.40402199e-01 -4.53806396e-01 -4.41702110e-01
  -1.13190695e-01 -1.17527026e-02  3.87079108e-01 -9.83597248e-02
  -9.20266039e-01]
 [ 1.11337405e-02 -1.51172858e-01 -2.56550353e-01 -6.17581620e-01
   3.26548922e-01 -2.06205942e-01  7.64077210e-02 -7.44057539e-02
  -4.08727483e-01  1.49696867e-01 -4.55489077e-01 -6.26691064e-01
  -2.58074086e-01  1.76431493e-02 -1.35019510e+00 -6.51552248e-01
  -4.27264507e-02  1.78746250e-01  2.21460820e-01 -1.55449571e-01
  -7.31954363e-01 -8.20460186e-01 -7.50715637e-01 -2.66518193e-01
  -1.09269575e+00]
 [-9.83635341e-01  1.89854826e-01 -4.12710168e-03 -3.98360352e-01
  -1.06531031e+00 -1.81022444e+00 -2.05927243e-01 -5.87394436e-01
  -5.04718370e-01 -5.61291419e-02 -3.11896789e-01 -1.26317819e-01
  -5.56152915e-01 -2.91460936e-01 -7.19468948e-01 -1.12130797e+00
  -5.00210738e-01 -1.06354390e-01 -1.07953695e+00 -6.17205950e-01
  -9.04240039e-01 -9.83166137e-02 -9.68409342e-01 -3.49778224e-01
  -6.11685994e-01]] 
Matrix V:
 [[ 0.00533118 -0.27679196 -0.37406267 ... -0.06848227 -0.48390184
   0.57169314]
 [-0.03561711 -0.60814097  0.40180082 ... -0.01860028 -0.2324768
  -0.53965522]
 [ 0.39679566 -0.02608081 -0.73028539 ...  0.02241231  0.65334733
   0.58225842]
 ...
 [ 0.56603357  0.2479394  -0.40454115 ...  0.03407429  0.15904543
   0.53843305]
 [-0.24858552 -0.38950686 -0.3688509  ... -0.2117455  -0.23825708
   0.02796388]
 [ 0.24950996  0.59213711 -0.65507228 ...  0.05713226  0.55305527
   0.11137295]] 
Matrix W:
 [[ 0.01472435 -0.2425166   0.52748374 ...  0.5630162   0.14903061
   0.40459205]
 [-0.2748     -0.24611391 -0.2610121  ...  0.02305128 -0.10676636
   0.62827097]
 [ 0.25735529  0.84626415  0.72792957 ... -0.08659645  0.01561521
   0.95921606]
 ...
 [-0.02071983 -0.42343725 -0.5227277  ... -0.84989042 -0.26577107
  -0.13152056]
 [-0.05971663  0.41279449  0.03600112 ... -0.27079258 -0.04178113
   0.05370001]
 [ 0.0155213  -0.07873848 -0.49809249 ...  0.26852537 -0.23003709
  -0.27069178]] 
===========================


Training model for 10 epochs
training set: 1000 sentences (batch size 100)
Optimizing loss on 1000 sentences
Vocab size: 2000
Hidden units: 25
Steps for back propagation: 5
Initial learning rate set to 0.5, annealing set to 5

calculating initial mean loss on dev set: 8.005974516119998

epoch 1, learning rate 0.5000	instance 1	epoch done in 49.57 seconds	new loss: 5.449484495800914
epoch 2, learning rate 0.4167	instance 1	epoch done in 49.35 seconds	new loss: 5.198745729082761
epoch 3, learning rate 0.3571	instance 1	epoch done in 52.99 seconds	new loss: 5.16463280089737
epoch 4, learning rate 0.3125	instance 1	epoch done in 50.45 seconds	new loss: 5.11608236133453
epoch 5, learning rate 0.2778	instance 1	epoch done in 50.23 seconds	new loss: 5.013985997112259
epoch 6, learning rate 0.2500	instance 1	epoch done in 50.20 seconds	new loss: 5.004948010405373
epoch 7, learning rate 0.2273	instance 1	epoch done in 50.35 seconds	new loss: 4.973612734611159
epoch 8, learning rate 0.2083	instance 1	epoch done in 50.13 seconds	new loss: 4.9600816255016955
epoch 9, learning rate 0.1923	instance 1	epoch done in 50.31 seconds	new loss: 4.96820505430481
epoch 10, learning rate 0.1786	instance 1	epoch done in 50.36 seconds	new loss: 4.933136297644739

training finished after reaching maximum of 10 epochs
best observed loss was 4.933136297644739, at epoch 10
setting U, V, W to matrices from best epoch

==========================
Matrix U:
 [[-5.72246022e-02  3.27271170e-01 -2.56167269e-01 -5.05603262e-01
  -3.72312000e-01  7.15350479e-02  1.20762520e-01 -1.75562063e-01
  -3.67611925e-01 -7.76119690e-01 -1.86848217e-01  9.32295968e-02
  -7.10918442e-02 -5.64417342e-02  4.40379301e-01 -5.64489085e-01
  -4.31519890e-01  2.59201888e-02  3.52212354e-01 -1.07775661e-01
  -9.61686716e-02 -3.00745758e-01 -1.15794174e-01 -9.89254880e-02
   9.89778606e-02]
 [-4.15833586e-01 -5.11038626e-01 -2.07816116e-01  4.43370708e-02
   3.78785201e-01 -1.38206308e-01  1.82390462e-01 -7.85089027e-01
   8.44976285e-02  1.19110803e-01  3.11503714e-02  2.92017857e-01
   3.94022666e-02  1.02653210e-01 -2.08416243e-01 -6.31390533e-01
   2.89843479e-01 -3.82488479e-01 -3.65647353e-01 -9.04227135e-02
  -2.87343121e-02 -2.00250470e-02 -1.31809770e-01 -1.87515246e-02
  -3.08204668e-01]
 [-2.25131334e-01  2.21994155e-02 -5.20132524e-01 -5.07247294e-01
  -7.91251788e-02 -7.85390240e-02 -3.38241641e-01 -1.69306502e-01
  -1.23911087e-01 -7.87375340e-01  4.05384860e-01  4.24704249e-01
   6.54795805e-01  1.12319055e-02  1.17924617e-01  2.08804616e-01
  -2.61547186e-01 -4.20011555e-02 -8.49234999e-03 -5.93320548e-01
   1.44771056e-01  6.75268035e-01  1.05959057e-01 -1.16819011e-01
  -2.93246266e-01]
 [-8.36840531e-02 -3.89792245e-01 -2.26062765e-01  4.49165087e-02
  -2.91415491e-01  1.76495003e-01  2.97197653e-01 -1.28526211e-01
   6.69544501e-02 -2.24178120e-02  3.39200063e-01 -1.45310596e-01
  -1.17894598e-01 -1.17555705e-02  1.42397726e-01 -2.81469729e-01
  -1.47373902e-01 -3.34161756e-01 -1.44429817e-01  2.21879571e-01
  -1.66347782e-01  4.42541917e-01 -7.27627498e-01 -2.41639405e-01
  -1.84509146e-01]
 [-1.37479406e-01 -5.23058255e-01 -1.97346754e-01 -4.50416644e-01
  -2.62083605e-01 -2.56654770e-01  1.92514085e-01 -3.46591216e-01
  -3.03576328e-01  8.93680272e-02 -9.96751634e-01  4.48867303e-02
   2.49360396e-01 -5.42920138e-01 -2.48578434e-01 -6.05654522e-01
  -5.51198691e-02  1.89530763e-01 -7.72497974e-02 -2.23780259e-01
  -3.80020244e-01 -4.93823044e-01 -5.02142138e-01 -1.21791597e-01
  -1.02468697e-01]
 [-7.14818804e-01  2.48712755e-01 -4.81530679e-01 -2.13549619e-01
  -1.13076267e-01 -1.06592373e+00  3.07288096e-01 -9.39696692e-01
   1.16464375e-01 -9.29397926e-01 -3.40666278e-01  2.56536454e-01
   3.73633589e-01 -1.98390137e-01  5.94046075e-02  1.74488459e-01
  -7.14838290e-01 -7.20515176e-01  6.05560079e-01 -4.94582212e-01
   1.27012742e-01 -2.98405783e-01 -2.12780898e-01 -1.65442466e-01
  -4.45828274e-01]
 [-1.12732170e-02  2.03089940e-01 -6.42472960e-02 -5.66626733e-01
  -7.05835439e-01 -3.13719324e-01 -7.12956874e-02 -4.51366984e-01
   2.64442606e-02 -3.31433687e-01  2.55977804e-01  4.19576732e-01
  -1.66146907e-01  3.68651931e-01 -1.10632997e-01 -5.30334624e-01
   3.46185614e-01 -4.96882801e-01 -3.21125413e-01 -4.63809090e-01
   1.05133169e-01  8.46091315e-03 -2.54145655e-01 -2.65618336e-01
   4.35574895e-01]
 [-2.01922295e-01  2.43020530e-01 -2.86348022e-01 -1.49833539e-01
  -1.34421601e+00 -6.44162452e-01  6.12736603e-01 -1.45633703e-01
  -1.58064683e-01 -2.37012723e-01 -7.34507053e-01  9.57087879e-02
  -5.92637159e-01 -1.49029257e-01 -8.59980084e-01 -1.05018478e+00
  -4.59277112e-02  3.85772345e-01 -2.96706345e-01 -3.64048267e-01
  -2.24664477e-01 -7.26959618e-01 -4.29941954e-02  4.98453494e-01
   2.30462894e-02]
 [ 1.70630051e-01 -3.77223492e-01  3.32061466e-01  8.57507347e-02
  -3.64172076e-01 -1.30458885e+00  2.06974997e-02 -7.76881799e-01
  -8.10435234e-02  4.98265752e-02  4.62551364e-02 -1.84405409e-01
  -1.82473855e-01 -2.00976339e-01 -2.36233598e-01  1.68695864e-02
   4.06999277e-03 -3.68420113e-01 -2.22655556e-01 -3.34282311e-01
  -8.71120058e-01  2.18460199e-01 -1.67297834e-01  7.16104387e-02
  -1.02846753e+00]
 [ 5.16015857e-01  2.48398688e-01 -2.15553990e-01 -2.05534087e-01
  -2.73109813e-01  5.43030530e-03 -2.11770739e-01 -1.13867014e+00
  -3.70961152e-01 -2.34928179e-01 -1.74651269e-01 -6.45716044e-01
  -3.51747785e-01 -1.06450782e-01  2.98354501e-01 -7.42680766e-01
  -2.57074348e-01 -6.33599536e-01  6.42778279e-03  3.84709616e-01
  -4.13262966e-01 -2.18367584e-01 -6.74279345e-01 -3.43466315e-01
  -3.94423108e-01]
 [ 4.37613561e-01 -1.58339198e-01  7.70661203e-01  2.93211631e-01
   4.88817391e-01  4.20582104e-01  5.38293798e-01  3.01837808e-01
   4.34385367e-01  5.08972167e-02  4.25886779e-01  2.73749593e-01
   4.07106840e-01  4.60162476e-01  9.02395251e-01 -3.56031857e-01
   1.49314750e-01  5.34839177e-01 -5.59152260e-02  9.04578356e-01
   3.98870274e-01 -1.60141618e-01  2.22299433e-01  3.72509252e-01
   5.74340976e-01]
 [-3.48395463e-01  1.67183893e-01 -3.62852745e-01 -1.35692452e-01
  -1.28283149e-01 -3.28614559e-01  1.20273170e-01 -7.00040653e-01
   6.01909118e-01  1.01606386e-01 -7.51589531e-01 -5.86262268e-01
   3.18304778e-01 -3.80238339e-01 -1.07724237e-01 -4.82640237e-01
  -1.97290869e-01 -1.33496586e-01 -1.12430700e+00 -2.11773237e-01
  -2.55241755e-01 -4.69429941e-01 -8.53087741e-02  3.56723838e-02
   3.65092695e-01]
 [ 1.12047331e-01  5.92169380e-02 -5.26157607e-01 -6.95034601e-01
  -5.26920018e-01 -1.79872600e-01  1.13001946e-01 -4.25658034e-01
  -1.64490425e-01 -6.80431468e-02 -5.93704567e-01 -1.36376788e-01
   1.87944931e-01  2.35169065e-01  6.43976786e-02 -3.01760157e-01
  -2.44359212e-01 -1.55421998e-01 -7.35279243e-02 -9.31122464e-02
  -3.30505750e-01 -2.72194594e-01 -6.69117463e-02 -3.93598906e-01
  -3.58970479e-02]
 [-5.11445244e-01  1.10887076e-01 -1.92897588e-01 -1.97475088e-01
   8.72104303e-02  2.01586578e-01  5.00107343e-01 -7.13318306e-01
   4.79231659e-01  1.02494419e-01  3.42366574e-01  3.37883274e-01
  -5.03906592e-01  3.87255831e-01  1.15000275e-01 -4.34271390e-01
  -4.70079194e-01  1.70790574e-01 -3.36462817e-01 -1.50300782e-02
   3.70613303e-01 -2.54252853e-01 -2.46491779e-01  2.25618981e-01
   6.82614693e-04]
 [ 5.96391066e-01  1.82899280e-01 -2.93422149e-01  1.31607746e-01
  -4.44033110e-01 -2.10536619e-01  6.28419971e-02  4.45345297e-02
  -2.29803964e-01 -6.17877143e-02 -1.07812526e-01  5.87519017e-02
  -2.14935714e-01  3.48255905e-01  5.30557953e-01  6.72179830e-01
   1.49985860e-01 -4.82159955e-01 -2.74303855e-01 -5.32111227e-02
  -4.81027056e-01 -7.51389895e-01 -4.02414872e-02 -1.68127355e-02
  -2.44175697e-01]
 [ 7.33606748e-01  2.75199280e-01 -4.74942471e-01 -1.39601547e-01
  -7.09574537e-01 -5.17570895e-01 -5.71736611e-01 -5.84855597e-01
   1.08407577e-01  1.52074351e-01 -3.98023745e-01 -2.73529661e-01
  -2.67133438e-01 -5.53276640e-01  5.83997786e-01 -9.99800700e-01
  -3.60535642e-01  1.49298101e-01  1.64233236e-02  2.90306073e-01
  -2.31579701e-01 -1.31731408e-01 -5.15126555e-01 -3.60810685e-01
  -2.84203233e-01]
 [ 3.70531324e-01  1.16285381e-01  1.64837393e-01  3.27586596e-01
  -5.60476060e-03  1.05245446e-01 -1.46783500e-01 -2.11989002e-01
   4.19220845e-01 -2.90099541e-01  2.53168244e-01 -3.95104771e-03
   2.20001292e-01 -1.65775354e-01 -3.89177538e-01 -8.66220838e-02
  -7.28972407e-02 -7.99113900e-01  3.54819022e-01  4.41791006e-01
  -6.28159682e-02  1.12412260e-01  2.38657754e-01  2.83887481e-01
   3.94556801e-01]
 [-1.31850269e-01  7.00170896e-01 -6.13573481e-01 -1.21321718e-01
   8.29378579e-02 -1.19189648e-01  2.60478484e-01 -4.09737484e-01
  -2.16580452e-01 -2.02663157e-01  2.45348506e-02 -1.23515251e-01
  -7.71877453e-01 -2.52501359e-01 -1.99370131e-01  1.47005044e-01
  -2.52968018e-01 -6.19632876e-01  9.06076880e-02  3.57542252e-01
   8.72122789e-02 -9.64184013e-01 -1.83004797e-01  4.20824477e-01
  -1.59659910e-02]
 [-2.30324446e-01 -2.88103917e-01 -3.70361595e-01  1.35442805e-01
  -4.68090769e-01 -3.66829934e-01 -3.99496953e-01 -7.04024118e-01
   3.56309252e-01 -3.48681113e-01 -2.57074871e-02 -3.38501539e-01
   3.93266683e-01  2.48789951e-01  2.06451283e-01 -1.05153421e+00
  -7.36827220e-02 -1.21352749e-01 -2.41384778e-01  1.30217113e-01
  -6.52459854e-01 -1.74424552e-01 -9.59263280e-02 -2.95281105e-01
   5.76589730e-01]
 [-1.66586299e-01 -4.58824851e-01  2.87836577e-01  4.71353391e-01
  -3.10630836e-01 -2.75532884e-01 -4.52061040e-01 -2.79486635e-01
   4.35544685e-02 -1.34482165e-01 -5.60576802e-01 -6.66818519e-01
  -5.98655119e-02  6.69713859e-02  2.12953115e-02 -3.43753225e-01
  -3.80353150e-02 -5.32636090e-01 -3.38171786e-02 -9.46971379e-02
  -1.25984124e-01 -3.77912372e-01 -2.02577301e-01  6.90969520e-03
   2.86900651e-01]
 [-1.75812957e-01 -9.66250767e-05  1.93343610e-01 -5.97913084e-01
  -5.95588105e-01  1.83581788e-01  1.03395214e-03  2.54976712e-01
  -8.24861167e-02 -5.44309812e-01 -1.44896992e-01 -2.28283241e-01
  -6.79012090e-01  2.75304873e-01  4.97970082e-01  2.13533891e-02
  -9.69351012e-02 -2.25456027e-01  2.03303613e-01 -3.53170920e-01
   2.05301955e-01 -4.77520824e-01  1.05246806e-01 -8.35658299e-02
  -1.46525240e-01]
 [-2.63805757e-01  1.42608976e-01  2.99121607e-01 -1.19825949e+00
  -5.02517993e-01 -2.76910191e-01 -7.23670525e-02 -5.55870346e-01
  -2.71742750e-01  2.20676497e-01 -4.52240446e-01 -1.92634714e-01
   7.62624752e-02  7.50374513e-02 -2.74641281e-01 -1.27120735e-01
   1.35970953e-01 -2.37644471e-01  4.18344693e-01  9.87886255e-02
  -1.39582515e-01 -5.40323800e-01 -6.86513851e-02 -3.24441481e-01
   5.91513387e-02]
 [-3.26756909e-01  1.37678534e-01  5.79234802e-02  3.58315101e-02
  -2.94331117e-02 -2.41166555e-01  2.37696090e-01 -1.59878095e-03
  -1.82538161e-01 -6.44640725e-02 -5.21554165e-01  1.33657750e-01
   1.66353497e-01 -9.69227311e-02 -3.78165779e-01  3.03233061e-01
  -4.40750950e-01 -2.54247423e-01  2.46111391e-01  5.11522601e-01
   3.26205729e-01  6.34005003e-02 -3.90886341e-01 -5.97354305e-01
  -5.69786513e-01]
 [-4.38052574e-01  1.53099683e-01  1.48705022e-01  5.42096992e-01
  -5.38507935e-02  3.92398872e-01 -4.18902792e-01 -4.69425060e-01
   2.19960214e-01 -3.60553746e-02 -4.78210987e-02  3.67252174e-01
  -4.64084722e-01 -2.76812023e-01 -4.18252857e-01 -9.49081994e-03
  -1.65298417e-01  5.40469849e-02  3.43323728e-01 -4.12023836e-01
   2.01904707e-01  3.82664519e-02 -3.08890091e-01 -1.27445350e-01
   1.33403362e-01]
 [-3.03288858e-01  2.18950181e-01  1.54122948e-01 -5.39074349e-01
   3.73717470e-01 -8.71851889e-02  3.51497291e-01 -4.82057987e-02
   5.16414250e-01 -3.40690981e-01 -2.99468297e-01  2.58290951e-01
  -3.79793723e-01  4.51671042e-02 -1.08558057e-01 -5.84511257e-01
   5.47087425e-02 -6.45023427e-01  1.25086422e-01  7.84975560e-02
  -1.87620692e-01 -6.71708356e-01 -2.00050573e-02 -5.14171235e-01
  -3.19131493e-01]] 
Matrix V:
 [[ 0.07121111 -0.1118962  -0.18969119 ... -0.12296952 -0.08801839
   0.31589334]
 [ 0.61394863  0.04765521 -0.4067555  ...  0.22433152 -0.24438356
  -0.14144892]
 [-0.27978996  0.39758307 -0.44720923 ...  0.13872816 -0.32150199
   0.46883532]
 ...
 [ 0.20873389  0.29390311 -0.40728939 ... -0.31479812 -0.40269819
  -0.31772648]
 [ 0.24649053  0.50423763 -0.3733987  ...  0.31973543 -0.20235066
   0.61280245]
 [ 0.5700325   0.05858506 -0.45136607 ... -0.26893642  0.26493479
  -0.23210798]] 
Matrix W:
 [[ 0.30315503  0.06923686  0.48124899 ...  0.73516771  0.52245064
   0.11411627]
 [ 0.05683245  0.54673609 -0.05822329 ... -0.37296408 -0.40965733
   0.03875416]
 [ 1.49416182  1.11366408 -1.20704187 ...  1.45672066  1.21653745
   1.1661797 ]
 ...
 [ 0.20347553 -0.08260864  0.2388278  ... -0.21613253 -0.0506942
  -0.54952642]
 [ 0.58124348 -0.19493365 -0.20816639 ...  0.32721601  0.09529857
  -0.76789893]
 [-0.01479604  0.23701994 -0.04064408 ... -0.07070046 -0.27385084
  -0.40592313]] 
===========================


Training model for 10 epochs
training set: 1000 sentences (batch size 100)
Optimizing loss on 1000 sentences
Vocab size: 2000
Hidden units: 50
Steps for back propagation: 0
Initial learning rate set to 0.5, annealing set to 5

calculating initial mean loss on dev set: 8.356998486898831

epoch 1, learning rate 0.5000	instance 1	epoch done in 40.51 seconds	new loss: 7.414658445440394
epoch 2, learning rate 0.4167	instance 1	epoch done in 41.10 seconds	new loss: 5.569864789334444
epoch 3, learning rate 0.3571	instance 1	epoch done in 40.56 seconds	new loss: 5.210865684902781
epoch 4, learning rate 0.3125	instance 1	epoch done in 40.68 seconds	new loss: 5.103003967137201
epoch 5, learning rate 0.2778	instance 1	epoch done in 40.69 seconds	new loss: 5.399067557252833
epoch 6, learning rate 0.2500	instance 1	epoch done in 40.87 seconds	new loss: 5.02768715246204
epoch 7, learning rate 0.2273	instance 1	epoch done in 40.55 seconds	new loss: 4.999068852531871
epoch 8, learning rate 0.2083	instance 1	epoch done in 40.70 seconds	new loss: 4.985172738594541
epoch 9, learning rate 0.1923	instance 1	epoch done in 40.19 seconds	new loss: 4.9682433666729295
epoch 10, learning rate 0.1786	instance 1	epoch done in 40.99 seconds	new loss: 4.970246293923974

training finished after reaching maximum of 10 epochs
best observed loss was 4.9682433666729295, at epoch 9
setting U, V, W to matrices from best epoch

==========================
Matrix U:
 [[-0.36290134  0.54556893  0.13511866 ...  0.10531322  0.28116118
   0.21789597]
 [-0.53671646 -0.88546626  0.50430868 ...  0.24394729 -0.30209731
  -0.13826774]
 [-1.13953038 -0.77889698 -0.10871759 ... -0.45668411 -0.54128137
  -0.25950838]
 ...
 [ 0.64146909  0.31187143 -0.0084645  ... -0.37334509  0.05629688
  -0.69480012]
 [ 0.26453682 -0.22346405  0.00574565 ...  0.17233252  0.10110352
   0.23919544]
 [ 0.2114846  -0.49102886 -0.18341053 ... -0.63610731 -0.05937439
   0.12554212]] 
Matrix V:
 [[ 0.67749835 -0.68959136 -0.67904776 ... -0.41907279 -0.05186476
  -0.21768984]
 [ 0.16199513  0.36421591  0.01089706 ... -0.07743814  0.28745821
  -0.03078787]
 [ 0.24717383  0.30340895  0.27150264 ... -0.3466112   0.01968787
  -0.28664377]
 ...
 [ 0.11533244  0.11151101 -0.72428363 ... -0.61118246 -0.25735922
  -0.05715669]
 [ 0.11025063 -0.33077414 -0.74319374 ... -0.3559393   0.4696124
   0.23580155]
 [-0.09349243 -0.04310524 -0.40184532 ... -0.07126353  0.19961583
  -0.30653018]] 
Matrix W:
 [[ 0.52071754  0.12885705  0.54498379 ...  0.42789974 -0.23145809
   0.42792874]
 [ 0.44442065 -0.23025235  0.38234041 ... -0.08282718  0.12168693
   0.45043651]
 [ 1.94565654  0.43312228 -0.46136253 ... -0.26574573  0.83404774
   1.09803424]
 ...
 [-0.02090751 -0.05610293  0.28012285 ... -0.26216177  0.31454986
   0.19221398]
 [-0.22201205  0.10542273 -0.13404493 ...  0.14687301 -0.1258705
   0.36755201]
 [-0.3144703   0.35938913  0.22548257 ... -0.26811461 -0.95754176
  -0.4613587 ]] 
===========================


Training model for 10 epochs
training set: 1000 sentences (batch size 100)
Optimizing loss on 1000 sentences
Vocab size: 2000
Hidden units: 50
Steps for back propagation: 2
Initial learning rate set to 0.5, annealing set to 5

calculating initial mean loss on dev set: 8.159437732854675

epoch 1, learning rate 0.5000	instance 1	epoch done in 49.81 seconds	new loss: 8.38412584078225
epoch 2, learning rate 0.4167	instance 1	epoch done in 50.38 seconds	new loss: 6.5512549395599535
epoch 3, learning rate 0.3571	instance 1	epoch done in 50.12 seconds	new loss: 5.82056685081018
epoch 4, learning rate 0.3125	instance 1	epoch done in 49.95 seconds	new loss: 5.510317298136842
epoch 5, learning rate 0.2778	instance 1	epoch done in 50.51 seconds	new loss: 5.2123033822731335
epoch 6, learning rate 0.2500	instance 1	epoch done in 50.24 seconds	new loss: 5.29302306494215
epoch 7, learning rate 0.2273	instance 1	epoch done in 50.17 seconds	new loss: 5.07121812317876
epoch 8, learning rate 0.2083	instance 1	epoch done in 50.39 seconds	new loss: 5.068860972324421
epoch 9, learning rate 0.1923	instance 1	epoch done in 50.48 seconds	new loss: 5.03590804124043
epoch 10, learning rate 0.1786	instance 1	epoch done in 50.37 seconds	new loss: 5.027570601491822

training finished after reaching maximum of 10 epochs
best observed loss was 5.027570601491822, at epoch 10
setting U, V, W to matrices from best epoch

==========================
Matrix U:
 [[-0.8881425   0.01629875 -1.43308959 ... -0.32314223  0.09033456
   0.1073997 ]
 [ 0.06894631 -0.45880873  0.84244349 ...  0.00427997  0.21662649
   0.31616067]
 [ 0.29200969 -3.53341613 -2.21802772 ... -0.09341161 -3.74825461
   0.22288049]
 ...
 [-0.45817335 -1.87828054 -0.42433776 ... -0.5107466  -1.06536344
   0.18775351]
 [ 0.76355803  0.27119787  1.24576208 ...  0.10626614 -0.05497836
   0.09266041]
 [-0.54672951 -0.7467949   0.28126441 ... -0.37117057 -0.54885676
  -1.20772116]] 
Matrix V:
 [[ 0.11291977 -0.07363583  0.42728931 ...  0.07031576  0.05686885
   0.03638538]
 [-0.24276467  0.09335683 -0.81837536 ... -0.57535616  0.73979066
   0.1015856 ]
 [-0.18513466  0.23721279 -0.91841995 ...  0.65274481 -0.2456204
  -0.16530849]
 ...
 [ 0.04237024  0.10018883 -0.49403343 ...  0.28792134 -0.13221815
  -0.35166356]
 [ 0.05623628 -0.23028922 -0.35622738 ...  0.37628009 -0.11121026
   0.25856387]
 [-0.83853563  0.23841761 -0.78924335 ...  0.12986434 -0.19886821
  -0.15336302]] 
Matrix W:
 [[ 0.46896808  0.59239713  0.33592987 ... -0.04814876  0.80898899
   0.43834734]
 [-0.16941415 -0.11087582 -0.34968241 ... -0.10077108  0.19862338
   1.42699083]
 [ 0.83480193  0.69026966  0.84403077 ...  0.85861174  1.4350441
   0.50646086]
 ...
 [ 0.58535547 -0.38784365  0.16337437 ... -0.01937633 -0.31294315
  -0.1524784 ]
 [-0.05604534  0.10398828 -0.52029199 ... -0.15575173  0.35577312
  -0.09077187]
 [-0.04293759  0.25017402  0.09064609 ... -0.13802729  0.31536229
  -0.14830866]] 
===========================


Training model for 10 epochs
training set: 1000 sentences (batch size 100)
Optimizing loss on 1000 sentences
Vocab size: 2000
Hidden units: 50
Steps for back propagation: 5
Initial learning rate set to 0.5, annealing set to 5

calculating initial mean loss on dev set: 8.623476566919857

epoch 1, learning rate 0.5000	instance 1	epoch done in 65.44 seconds	new loss: 7.512639839083601
epoch 2, learning rate 0.4167	instance 1	epoch done in 64.56 seconds	new loss: 6.097188652710518
epoch 3, learning rate 0.3571	instance 1	epoch done in 64.41 seconds	new loss: 5.232716284101785
epoch 4, learning rate 0.3125	instance 1	epoch done in 65.14 seconds	new loss: 5.109902865043581
epoch 5, learning rate 0.2778	instance 1	epoch done in 64.71 seconds	new loss: 5.099857779610886
epoch 6, learning rate 0.2500	instance 1	epoch done in 64.42 seconds	new loss: 5.054346828518153
epoch 7, learning rate 0.2273	instance 1	epoch done in 64.67 seconds	new loss: 5.016407417634956
epoch 8, learning rate 0.2083	instance 1	epoch done in 64.62 seconds	new loss: 4.995120137633387
epoch 9, learning rate 0.1923	instance 1	epoch done in 64.39 seconds	new loss: 4.978735220107267
epoch 10, learning rate 0.1786	instance 1	epoch done in 64.75 seconds	new loss: 4.964726004078286

training finished after reaching maximum of 10 epochs
best observed loss was 4.964726004078286, at epoch 10
setting U, V, W to matrices from best epoch

==========================
Matrix U:
 [[-0.15981893 -0.49844219 -0.87393025 ... -0.57694137 -0.3675512
  -0.61620935]
 [-0.58076496 -0.44499493 -0.05222175 ...  0.45382274 -0.17990261
  -0.20619208]
 [ 0.4737994  -0.18670219 -1.37400323 ...  0.18759682 -0.32412206
   0.17795658]
 ...
 [ 0.16515767 -0.375381   -0.61092594 ... -0.43260048 -0.07354664
   0.71764754]
 [-0.47356016  0.17517377 -0.12717698 ... -0.13974934 -0.37085438
  -0.7118144 ]
 [-1.54612721 -0.22793213 -0.3124117  ... -0.84453101  0.13363911
  -0.88456976]] 
Matrix V:
 [[-0.23519338 -0.53575194 -0.61148121 ...  0.02080087  0.28345833
  -0.28628063]
 [ 0.22317875  0.65582985  0.25859346 ...  0.11345816 -0.37568888
  -0.60286324]
 [ 0.21245909 -0.158436   -0.68602494 ... -0.20229927  0.01539929
  -0.15177339]
 ...
 [-0.04701857  0.00504755  0.1374999  ... -0.22796172 -0.32452022
  -0.45903417]
 [-0.1775054  -0.4348643  -0.93748757 ...  0.35429697  0.18440676
  -0.37840055]
 [ 0.49521101  0.28746595 -0.22911511 ...  0.38381061 -0.61449835
   0.15919171]] 
Matrix W:
 [[-1.99383563e-01  2.59378244e-01  4.46681862e-02 ...  2.43627927e-01
  -1.43931833e-01 -7.42623133e-02]
 [ 3.27235244e-01  1.82000792e-01  4.77020472e-01 ...  1.62882755e-01
   3.24526455e-01  8.62940366e-02]
 [ 2.59688845e-01  2.08330440e-01  1.27260375e+00 ...  4.38802773e-01
   6.24833772e-04  2.82985708e-01]
 ...
 [ 6.96854728e-01 -3.38377077e-01 -3.37525451e-01 ... -4.25220113e-02
   1.19657955e-01 -4.33878358e-01]
 [ 2.79758391e-01 -1.64120854e-01  1.72415134e-01 ... -2.98871907e-01
  -1.73961762e-01 -6.74674958e-02]
 [-6.43198665e-02  1.04834169e-01 -2.62677518e-01 ... -4.62824025e-01
   1.64866423e-01  2.06554624e-01]] 
===========================


Training model for 10 epochs
training set: 1000 sentences (batch size 100)
Optimizing loss on 1000 sentences
Vocab size: 2000
Hidden units: 25
Steps for back propagation: 0
Initial learning rate set to 0.1, annealing set to 5

calculating initial mean loss on dev set: 7.899192285877461

epoch 1, learning rate 0.1000	instance 1	epoch done in 34.87 seconds	new loss: 5.777144924874237
epoch 2, learning rate 0.0833	instance 1	epoch done in 35.70 seconds	new loss: 5.535528956995093
epoch 3, learning rate 0.0714	instance 1	epoch done in 35.40 seconds	new loss: 5.433896499634995
epoch 4, learning rate 0.0625	instance 1	epoch done in 35.86 seconds	new loss: 5.380447450873842
epoch 5, learning rate 0.0556	instance 1	epoch done in 35.20 seconds	new loss: 5.324680510629727
epoch 6, learning rate 0.0500	instance 1	epoch done in 34.89 seconds	new loss: 5.289558195250442
epoch 7, learning rate 0.0455	instance 1	epoch done in 35.06 seconds	new loss: 5.2601985857653535
epoch 8, learning rate 0.0417	instance 1	epoch done in 35.15 seconds	new loss: 5.2367257728802326
epoch 9, learning rate 0.0385	instance 1	epoch done in 35.06 seconds	new loss: 5.214215013583637
epoch 10, learning rate 0.0357	instance 1	epoch done in 35.12 seconds	new loss: 5.195890896884487

training finished after reaching maximum of 10 epochs
best observed loss was 5.195890896884487, at epoch 10
setting U, V, W to matrices from best epoch

==========================
Matrix U:
 [[-2.30349511e-01 -3.62215302e-01 -6.79770117e-02  1.93690797e-01
   3.65851030e-02 -6.30489098e-01  5.82608991e-01  1.84578542e-01
   3.28947467e-01  3.97915946e-01 -2.28112127e-01 -1.46954793e-01
   2.97276313e-01 -2.13455204e-01 -2.27038125e-02  1.18886065e-01
  -4.99998382e-01  5.47006120e-03  1.19411138e-01  2.08695913e-01
  -3.57883403e-02 -1.31394910e-01 -7.98720883e-02  2.72890408e-01
   3.87224038e-01]
 [ 3.84399116e-01 -7.89465770e-01  1.20460551e-01  1.00287610e-01
  -3.65431126e-01  2.37605475e-01 -4.10731856e-02  5.03297442e-01
  -2.29481250e-01  4.48636871e-01 -4.91326199e-01  4.64243149e-01
   5.41918019e-01 -2.45220332e-01 -2.27915964e-01  1.53660860e-01
   6.01866076e-01 -2.94919709e-01  4.65088769e-01 -1.80753104e-01
  -1.16820668e-02 -2.00024547e-01  1.23572872e-01 -1.92922193e-01
   2.01627640e-01]
 [-2.86298456e-01  8.74191246e-02 -3.12610751e-01  9.74239323e-02
  -2.75727119e-03 -1.47773752e-01  1.82782580e-01 -3.69408189e-01
   8.40295031e-02 -5.42481418e-01  6.00071822e-02 -8.87457005e-02
   2.17157682e-01  2.69924025e-01  4.46070387e-02 -4.93235418e-01
  -3.03011754e-01 -7.32148752e-01 -2.67624455e-01 -6.40781208e-02
  -3.40035091e-02  6.10514320e-02 -5.88821084e-01 -4.41935234e-01
  -7.84509615e-02]
 [ 1.64830495e-01  8.05435830e-02  1.73392272e-01 -7.40185611e-01
  -7.03513344e-01  4.15401006e-01 -4.01889539e-01  4.33431944e-02
  -1.24182556e-01  5.09181177e-01  1.87630017e-01  2.06685601e-01
  -1.34736943e-01 -2.67129398e-01  1.69086607e-01  1.20099466e-01
   3.91112253e-01 -6.42421271e-02 -3.16372813e-01 -1.42160363e-01
  -4.11035039e-01  1.90692424e-01  8.64586213e-02 -3.64809412e-02
   1.15796333e-01]
 [ 3.16693270e-01 -3.77677617e-01  7.05434866e-02 -4.10437365e-01
   3.52374893e-01  5.71965195e-01 -2.68864211e-01 -2.20575653e-02
   7.22029948e-01 -2.97605018e-01  4.91058533e-02  2.74376203e-01
   3.19635639e-01 -2.32707083e-01  1.81428212e-01  2.05826323e-01
  -1.72698329e-01 -2.82031553e-01 -2.81732986e-01  2.81627009e-01
  -3.78550366e-01 -1.77012012e-01 -5.41669277e-01 -4.40102678e-01
  -2.86853879e-01]
 [ 4.97050158e-01  2.55117012e-01 -1.05637628e-02  1.59895556e-01
   3.05898403e-01  2.62623388e-03  3.76722540e-01  2.22514983e-01
   2.90921829e-01  2.97202384e-01  6.66708376e-01  9.40520669e-02
   3.04984984e-01  3.03823830e-01  5.31888191e-02  3.51887430e-01
  -1.94363694e-01 -1.56020703e-01  4.30024400e-01  2.39730544e-01
   3.83916096e-01  1.57186558e-01  1.15452239e-01 -6.34772370e-02
  -1.03431522e-01]
 [-5.32908744e-02 -1.07553323e-01  3.90993580e-01  5.15472364e-01
  -7.41746413e-01  9.45434592e-02  1.80208862e-02 -1.20489314e-01
  -2.13482907e-01  3.43032776e-01 -2.22820282e-01 -2.35660541e-01
  -2.01709784e-01  3.13857510e-01 -1.76031587e-01 -1.28024690e-01
  -1.40943057e-01 -5.72413621e-02 -5.31578572e-01 -4.27445405e-01
  -5.36141758e-01 -2.20102831e-01 -4.50210720e-01 -3.65047803e-01
  -6.21398298e-02]
 [-1.97455356e-02 -3.82251902e-01 -3.99737587e-03 -1.88016803e-01
   3.46535649e-01 -4.24959637e-01 -4.15194243e-01  1.82036694e-01
   2.85055606e-02  3.65322120e-01  1.05218451e-01  4.27593774e-01
  -4.50651844e-02 -1.19229263e-01 -2.80752368e-01  3.99553181e-02
  -5.50372103e-02 -2.04536466e-01 -1.48096671e-01 -3.58813648e-01
   6.36947491e-01  4.13630533e-01 -5.01510804e-01  1.33472635e-01
   9.19114340e-02]
 [ 1.74062405e-01 -1.40086804e-01 -1.73107480e-01 -5.84270838e-02
  -8.04464477e-02  1.53692955e-01  2.42153909e-01  4.46744491e-03
   1.09226132e-01  1.20783084e-01 -3.70495944e-01 -4.40062782e-01
  -7.84251850e-02 -8.25004422e-02 -8.16753936e-02  1.21104215e-01
   4.39630198e-01 -3.77993451e-01 -2.86779744e-01  1.21624181e-01
  -3.08916616e-01  2.10086204e-01 -2.22111946e-01 -4.34282085e-01
  -1.47669697e-02]
 [ 3.66497357e-01 -5.26760511e-02  2.82558380e-01  2.92241887e-01
   9.18825872e-02  9.06782570e-02  1.96728924e-01  1.78147247e-01
   5.20716621e-01  5.43268286e-01 -1.28641254e-01  3.48363360e-01
   7.44047800e-01 -5.62143363e-01  3.80770450e-01  1.75909499e-01
   7.98157113e-02 -2.57925075e-01  8.32236353e-02  1.44306648e-01
   4.13474313e-02  1.26152638e-01  3.32314915e-01  7.60778334e-01
   1.82025462e-01]
 [ 4.16006922e-01 -8.11040788e-02 -1.18468109e-01  2.27130420e-01
  -8.07441479e-02 -4.96976616e-01 -4.28240119e-01  1.72456730e-01
   1.96089210e-01 -2.43777077e-01  3.66863643e-01 -2.12483878e-01
  -2.24857301e-01 -2.14342085e-03  7.85098304e-02 -2.99725021e-01
  -5.78324672e-01 -4.40746962e-01  9.67837486e-02  1.40858640e-01
  -2.21149217e-01  3.56702269e-01 -2.03803527e-03 -3.97367823e-01
  -2.09962476e-02]
 [-9.37738368e-01 -9.61018933e-02  1.08582995e-01  3.63304106e-01
  -4.01325446e-01 -2.93209716e-01 -7.37849346e-02  3.40890849e-01
  -3.85103388e-02  6.68064190e-02  1.32240182e-01  1.72216286e-01
  -1.94620985e-01  3.25744896e-01  4.29522896e-01 -4.86621082e-02
  -1.24444057e-01  1.43395426e-01  2.52785106e-01 -3.73016543e-02
   4.82586670e-01  2.97242766e-01  2.63321992e-01 -5.41126886e-01
  -1.66418595e-01]
 [-1.18092360e-01 -3.92428818e-01 -3.31471754e-01 -3.16486681e-01
  -3.04630832e-01 -5.68585281e-01 -9.58532864e-03 -3.66392649e-01
  -6.98412085e-01  6.06895019e-01 -2.41875648e-01 -2.17622284e-01
  -1.49474483e-01  2.26906142e-01  4.13490475e-01 -5.04655789e-01
  -1.55887615e-01 -7.16161038e-01 -1.36313795e-02  4.08754011e-01
  -3.19027548e-01  2.41425153e-01  1.56753086e-01  5.39228849e-01
   1.45737003e-01]
 [ 1.32092937e-03  2.40440392e-01  1.90355476e-01 -3.76335575e-02
  -1.87424829e-01 -9.30606435e-02  1.50322057e-01  5.26722263e-02
   1.72143647e-01 -2.40455491e-01  3.35553031e-02 -1.95411816e-01
   1.89017604e-01 -2.93495567e-01 -6.37861463e-01 -1.65115030e-01
  -5.60613175e-02 -1.09098061e-01  5.67683521e-03 -3.72413437e-01
   1.39326020e-01 -4.40008582e-01  5.75724552e-02 -3.57293042e-01
   1.73311111e-01]
 [-5.31180083e-01 -2.18809855e-01 -2.96280752e-01  1.29417711e-01
  -4.65850899e-01 -2.93655625e-01  8.93380505e-02  1.14643350e-01
  -9.29043358e-02  1.03267038e-02  2.44132644e-01 -4.16407578e-01
   4.44940362e-01  5.02913539e-01  1.94074546e-02 -9.72712858e-02
  -1.20178754e-01  2.32844521e-01 -4.99530721e-02  1.06245262e-01
   7.40148346e-04 -1.41940949e-01  5.71763436e-01  6.00810514e-03
  -2.51820908e-01]
 [ 2.63610212e-01  4.50812057e-01  4.50606409e-03  2.49171200e-01
   4.00155541e-01  4.77698118e-01  1.08697535e-01 -1.16481655e-01
   1.75688851e-01  1.67569429e-01 -3.27679511e-01  6.91978731e-01
   1.34609779e-01 -6.00561168e-02  1.36347486e-01  3.29589614e-01
   3.05627627e-01 -9.10308773e-02  3.99266849e-01  2.89589777e-01
   1.89977149e-01  7.79595844e-02  3.46675565e-01 -2.91718524e-01
   4.38209884e-01]
 [ 1.24430493e-01  3.91506232e-03  1.02069420e-01 -1.49332168e-01
  -3.73433856e-02 -3.62673970e-02 -4.75628076e-01 -2.44643466e-01
  -1.50021645e-01  4.32379671e-02 -2.07486050e-01 -2.68030522e-01
   2.24032406e-01  1.33608879e-01 -7.39591559e-01 -7.10737668e-02
   1.36884298e-01  2.28408186e-01 -3.05110222e-01  5.84288829e-01
   2.91704163e-01 -8.95642159e-02 -2.87065040e-02 -2.12227228e-01
  -9.30651532e-02]
 [ 1.25492476e-02  2.02083812e-01 -1.64311306e-01  1.16549063e-03
   3.71884695e-01 -6.03578349e-03  2.88389328e-01  2.01221364e-01
  -3.26920360e-01  2.15917568e-01 -1.14709919e-02 -2.54061772e-01
  -1.53807067e-01 -2.64783336e-01  2.98459963e-01  4.94454232e-01
   4.82294908e-01 -2.62419040e-01  2.60824767e-02  2.64574793e-01
   3.03852444e-01  4.72950718e-01  7.05183739e-03 -2.53058981e-02
  -2.28086832e-01]
 [-4.63820944e-01  1.49659996e-01 -4.43958106e-02  1.33690682e-01
  -4.33671576e-01 -1.58803027e-01 -3.25408485e-01  3.03214627e-01
  -4.12149112e-01 -1.82902583e-01  4.26593148e-01 -1.82897237e-02
  -1.78977697e-01  2.69299858e-01  7.20709079e-02 -6.75608899e-02
  -4.69353095e-01  4.23332240e-02  5.67599570e-01  1.31364492e-02
   1.03957583e-01  2.99964051e-01  4.18473025e-01  3.96681074e-01
   4.64743613e-02]
 [ 4.61689211e-01 -1.44106901e-01 -3.71803998e-01 -2.05348229e-01
  -2.68906419e-01 -2.50033699e-01  2.20512108e-03 -2.94628136e-01
   1.75551304e-01  4.84855531e-01  1.58945044e-01 -2.89316345e-01
   1.02133211e-01  5.66418913e-02  2.38947472e-01  3.17875070e-01
  -8.30384171e-01 -3.07100338e-01  7.74005515e-02  3.04630825e-01
  -1.91512043e-01 -1.95507491e-01  2.99545498e-02  3.41586624e-02
  -2.08378706e-01]
 [-2.69261918e-01 -1.06832093e-01  3.26581300e-01 -3.26521395e-01
  -3.04236451e-01 -2.63903695e-01 -2.91759459e-01 -8.27996297e-03
   1.05131497e-01  2.81303539e-01  1.24920771e-02 -2.81342974e-01
   1.61125695e-01 -2.85642709e-02 -4.45402398e-02  1.43201673e-01
   4.97550912e-01  1.46032144e-01 -7.84607753e-02  2.44898846e-01
   1.66903753e-01 -1.71089143e-01 -4.73223764e-01  3.99121363e-01
   1.91272590e-01]
 [ 9.84922969e-02  1.94788771e-01  3.90968923e-01  1.99545186e-01
   5.43758537e-01 -9.49873960e-02 -6.47890262e-01  2.27458481e-01
   1.77194933e-01 -5.04173762e-01  3.36939151e-01  4.03383816e-01
  -1.47427222e-02 -2.35472772e-02 -6.89342847e-02  4.55844262e-01
  -8.92011725e-02  1.16641157e-01  6.53278915e-01  3.25499957e-01
  -3.03821980e-02  3.21852052e-01 -1.47682665e-01  1.21969169e-01
  -4.25631754e-02]
 [ 3.97595268e-01 -1.05022621e-01 -4.02315273e-03 -6.04695428e-02
   2.77758699e-01 -5.49611190e-02 -1.17162499e-01  1.47862369e-01
  -1.39649894e-01  7.93535575e-02 -6.81847334e-02  2.77404626e-01
   4.49218405e-01 -2.08676189e-01 -5.40029222e-01 -2.26531022e-01
   1.44850154e-01  4.47733026e-01 -4.38488843e-02 -1.63746369e-01
  -2.55564387e-01  5.62969991e-01  1.59543554e-01 -3.55160219e-01
   4.46665774e-01]
 [ 3.41803671e-01 -1.16764915e-01 -8.49565112e-02  1.35885899e-01
  -4.79102012e-01  2.77305689e-02 -3.13436080e-01 -2.05171823e-02
  -4.76028379e-01  3.54838912e-01 -1.59715612e-01  4.41154925e-01
   9.27413495e-02  2.20868986e-01  1.13138576e-01 -4.36896234e-02
  -2.95042729e-01  4.47576580e-01  1.34704147e-01  5.06137246e-02
  -6.13089254e-01  5.79044205e-02  1.05242825e-01  9.61741871e-02
  -5.13435482e-02]
 [-2.01181759e-01  6.22574600e-02  3.20257470e-01 -4.43168387e-01
   3.02459541e-01  2.17079116e-01  4.82456095e-01  4.16281467e-01
  -2.47299581e-01  9.12029178e-02 -1.66663311e-01 -3.81107142e-01
  -4.23466359e-01 -8.07970533e-02  1.92760917e-01 -6.81083137e-01
  -5.00380889e-01 -1.27566792e-02  1.14100858e-01 -1.44734685e-01
  -2.14086542e-01 -2.94035093e-01  3.41186536e-01  1.68749923e-01
  -2.31194794e-01]] 
Matrix V:
 [[-0.46429322  0.66216047  0.14019025 ...  0.17587027  0.13637079
   0.07873042]
 [-0.59666153 -0.04006185 -0.1624912  ... -0.17638437  0.07164873
   0.14285346]
 [ 0.33137031 -0.11700741  0.10144307 ...  0.33104662 -0.00350988
   0.35931772]
 ...
 [-0.17326839  0.32018289 -0.55679683 ...  0.29413989 -0.47829499
   0.22321665]
 [-0.04033952  0.36776586 -0.025413   ...  0.018682    0.55385487
  -0.41152991]
 [-0.01864488  0.38952099  0.10412218 ...  0.11647018 -0.24439929
  -0.25648128]] 
Matrix W:
 [[ 0.41214449 -0.2748452  -0.01735737 ...  0.43098045 -0.13951277
  -0.0702117 ]
 [-0.1567875   0.03326475 -0.06426734 ...  0.41185745  0.30264237
   0.25690636]
 [ 0.39418321  0.96892298  0.89369109 ...  0.95878981  0.03965874
   0.43378333]
 ...
 [ 0.2745215   0.21923188  0.09261328 ...  0.16955355  0.07314552
  -0.3542756 ]
 [ 0.48136367 -0.06846861 -0.13491041 ...  0.2467037  -0.12773455
  -0.3611066 ]
 [ 0.01775758 -0.14925364 -0.10313241 ...  0.11736922 -0.1153642
  -0.10013387]] 
===========================


Training model for 10 epochs
training set: 1000 sentences (batch size 100)
Optimizing loss on 1000 sentences
Vocab size: 2000
Hidden units: 25
Steps for back propagation: 2
Initial learning rate set to 0.1, annealing set to 5

calculating initial mean loss on dev set: 7.9166664024617655

epoch 1, learning rate 0.1000	instance 1	epoch done in 40.89 seconds	new loss: 5.837014830649937
epoch 2, learning rate 0.0833	instance 1	epoch done in 41.04 seconds	new loss: 5.534050025747921
epoch 3, learning rate 0.0714	instance 1	epoch done in 40.93 seconds	new loss: 5.427819483331139
epoch 4, learning rate 0.0625	instance 1	epoch done in 40.92 seconds	new loss: 5.363452438839512
epoch 5, learning rate 0.0556	instance 1	epoch done in 41.29 seconds	new loss: 5.318469608561559
epoch 6, learning rate 0.0500	instance 1	epoch done in 40.86 seconds	new loss: 5.283908658276463
epoch 7, learning rate 0.0455	instance 1	epoch done in 41.02 seconds	new loss: 5.254786936040149
epoch 8, learning rate 0.0417	instance 1	epoch done in 40.82 seconds	new loss: 5.230270284969352
epoch 9, learning rate 0.0385	instance 1	epoch done in 40.85 seconds	new loss: 5.209179777191632
epoch 10, learning rate 0.0357	instance 1	epoch done in 40.91 seconds	new loss: 5.190748163295862

training finished after reaching maximum of 10 epochs
best observed loss was 5.190748163295862, at epoch 10
setting U, V, W to matrices from best epoch

==========================
Matrix U:
 [[-1.26508999e-01  1.37211754e-02 -5.77914061e-01 -2.77956225e-01
   1.70925482e-01  4.62617558e-01 -5.13060484e-01  2.98072113e-01
   8.93703139e-02  3.40173184e-01 -4.09613322e-01 -3.45640042e-02
  -1.92340885e-01  2.65043276e-01 -4.71656560e-01 -8.75206027e-01
  -2.96143843e-01  1.93048528e-01 -6.70584434e-01 -1.88810041e-02
  -2.74547515e-01  1.26048446e-01 -4.71446120e-01  3.33597782e-01
  -1.56990894e-02]
 [ 2.92779374e-01 -1.52868857e-01 -3.54051108e-01 -4.86832360e-01
  -3.05544933e-01 -2.45012506e-01  3.07185073e-01  2.78370016e-01
  -3.94727663e-01  5.47836954e-01 -4.01025053e-01 -1.87314616e-01
  -1.26930531e-02  1.12663959e-02 -1.94677243e-02  5.08737702e-01
   2.36566756e-01 -1.44032500e-01 -4.79373778e-02  5.71820413e-02
  -1.09098134e-01 -4.94009939e-02 -8.51084213e-02 -1.76834217e-01
  -3.64169787e-01]
 [-1.52553293e-01  6.09720571e-01  1.09360717e-01 -5.86901237e-02
   1.65997222e-01 -2.94855047e-01 -2.85741096e-01  6.95657555e-01
  -7.73820546e-02 -6.09019993e-01  4.52329582e-01  2.11664227e-02
  -7.64563586e-02  2.07544316e-01 -4.57662041e-02 -4.38614529e-01
  -1.67610058e-01 -1.64910254e-03  2.04473544e-01  3.29796269e-02
  -3.40926471e-01 -4.07073849e-01 -7.26953185e-01 -5.28939945e-01
   2.72135741e-01]
 [-2.48715981e-01 -3.31795532e-01  3.77894410e-01  1.74159305e-01
   4.04203546e-01  2.55933452e-01  1.57784103e-02  1.65449006e-01
  -4.67389405e-01  6.57218292e-02  7.54986733e-02 -1.76354950e-01
  -4.67289229e-02 -8.06433684e-02 -3.22617417e-01 -3.01458060e-01
  -4.96789918e-01  7.47418879e-02 -4.07801777e-01  1.31778279e-01
  -4.74918925e-01  1.78283575e-01 -1.85994413e-01  4.61876887e-01
   5.78798670e-01]
 [-2.90012419e-01  4.56246820e-01 -2.73254657e-01 -3.82914151e-02
  -5.52489483e-02  7.38998798e-02 -3.46766156e-01 -9.58214062e-01
  -2.65502776e-01  1.00882120e-01  1.79985283e-02 -3.47172223e-01
  -1.66548925e-01 -5.47680254e-01 -3.16151115e-01 -5.28359705e-03
  -2.10935206e-01 -2.39849431e-01 -2.40368788e-01  1.97907069e-01
   2.35674540e-01  1.61001078e-01  5.16086162e-01  6.22552865e-01
  -6.70512338e-01]
 [ 5.95917327e-02 -4.82208002e-01  4.10074710e-01 -3.63956568e-01
  -2.65408742e-01  2.09822989e-01 -6.29869093e-01  1.36651250e-01
   7.37470977e-02  2.48859281e-01  3.25077736e-01  2.31598865e-01
   1.10021815e-01 -3.02995149e-02 -1.00633941e+00  2.23810074e-01
  -4.47870933e-01  7.18173632e-03 -2.95122834e-01 -4.16292397e-01
   1.05832179e-01 -2.94393005e-01 -4.89630289e-02 -6.16254503e-02
  -9.14783134e-01]
 [ 7.78393878e-02  9.73418919e-01 -2.36808000e-01 -4.27118570e-01
   1.08144815e+00 -5.81780831e-04  4.65300917e-01  2.34915458e-01
  -1.49017291e-01  7.28198406e-03  5.38128262e-01 -2.79639113e-02
  -3.39717899e-01  1.51922686e-01 -4.20241304e-01  1.97531233e-02
  -1.18893489e-02  1.75768550e-01  6.76524038e-02 -4.64632479e-01
   3.05624011e-01  6.37896694e-01  4.93978783e-01 -3.48797131e-01
  -7.19211710e-02]
 [-3.96676351e-01 -2.69243238e-01  1.33437198e-01  4.78222862e-01
   5.84854047e-01  1.08253651e-02  4.58005785e-02  3.90725598e-01
   2.30923632e-01  3.77925104e-01  2.54385292e-01 -2.39610670e-01
   1.35419978e-01  2.87242859e-01  9.40822422e-03  1.36596540e-01
   4.10182332e-01 -1.58985291e-01 -2.75457603e-01  3.27139358e-02
   6.94536017e-01  1.72751311e-01  2.15385669e-01 -4.53106742e-01
  -1.70063688e-01]
 [ 4.16365895e-01  5.43580658e-02  2.41855241e-01  2.46410331e-01
   1.02739292e-01  2.03964522e-01 -1.87966644e-01 -1.72412231e-01
   8.03199815e-03 -3.70773465e-02 -2.64951229e-01 -1.88344129e-01
  -1.72369400e-01  8.25151887e-02  5.48544595e-01  1.37201563e-01
   3.26895285e-01 -4.89955321e-01 -5.33171495e-01  2.02101264e-01
  -1.76660619e-01  2.24647157e-01 -2.91915206e-01  2.25292418e-01
  -2.55544985e-02]
 [-3.04059106e-01 -1.96794315e-01 -2.38987543e-01  5.22298634e-01
  -5.36882095e-01 -5.52614103e-02  3.19803002e-01 -2.89536272e-01
  -2.36885889e-01  5.06459554e-01  4.59933873e-01 -6.50192881e-02
   2.65042602e-01 -1.44016419e-01  5.04519268e-01 -5.05336680e-01
  -1.45864350e-01 -4.50117961e-01 -7.77457171e-02 -3.52577074e-01
  -1.59084765e-01 -1.48765313e-01  5.14412114e-02 -6.21982596e-01
  -3.21813717e-01]
 [-3.83173126e-01 -1.09939768e-02 -6.08351853e-02 -2.37346026e-01
   8.90682054e-02  2.63236532e-01 -6.10532884e-02 -9.65574650e-02
   5.02745844e-01 -3.64141471e-01 -4.29690543e-01  1.94368175e-01
  -1.85011522e-01  5.67665545e-01  2.79228789e-01 -5.71722855e-02
  -4.41062496e-01 -2.66041256e-01  7.23859213e-02 -9.53771219e-01
  -2.45493180e-02 -1.01066139e-01  7.13360364e-02 -7.41837873e-02
   2.04775528e-01]
 [-1.17085952e-01  1.39992227e-01  2.89683811e-01  2.79416556e-01
  -2.44781914e-01 -4.93626046e-01 -6.85278382e-01 -1.64434699e-01
  -2.35909353e-01 -3.54268162e-02 -1.40671863e-01  1.48271062e-01
  -2.79441945e-01 -1.66036845e-02  2.83734243e-01  3.36113558e-01
  -2.12796969e-01 -1.74563234e-01  1.72158195e-01 -5.61141418e-02
  -4.92747326e-01  2.57710487e-01  3.13117052e-01  1.19112303e-01
   3.82261990e-01]
 [ 5.33768260e-01  2.18028468e-01  5.30054510e-01 -3.51943723e-01
  -3.40825673e-01 -2.71339501e-01  3.55813889e-01 -8.90788231e-02
   4.33867453e-01  8.60452000e-01 -1.13759221e-03  1.79088558e-01
   4.27632994e-02  1.75646226e-01 -9.53048865e-02 -3.16725361e-01
   2.25498975e-01 -2.35542579e-01  9.51028684e-03  1.15904653e-01
   4.57215474e-01  1.40350619e-01  2.91260715e-01 -7.67655414e-01
   1.95920814e-01]
 [ 3.96274441e-02  1.44496439e-01  4.69316940e-02  5.11516660e-01
   8.29229676e-02 -6.22764595e-01  3.48244055e-01  3.07701350e-01
  -1.44892403e-01 -7.28278232e-02 -3.40860536e-01 -4.69071362e-01
  -2.79883993e-02 -2.04658286e-01 -4.40185390e-01 -4.55786611e-01
   3.58996887e-02  3.64669427e-01  1.36010434e-01 -3.39973392e-01
  -2.39914317e-01  1.93133968e-01  3.97752861e-01  1.56161994e-01
   1.44847842e-01]
 [ 4.87374011e-02  2.76437938e-01 -7.03474423e-02 -3.09437409e-01
   3.71601923e-01  1.11895072e-01  2.49981012e-01 -1.18829320e-02
   2.36091091e-01 -6.18218314e-02  8.35904251e-02  4.25986972e-01
  -5.88440546e-01  2.11573840e-01 -1.61929412e-01 -2.44190838e-02
   4.08695608e-01  1.67251627e-01 -6.68363412e-02 -8.41337466e-02
   8.82627251e-01  1.03132421e-01  3.35079594e-01 -3.31454601e-01
  -6.19651369e-01]
 [-1.17217003e-01 -1.17799597e-01 -6.38892047e-01  9.24978290e-02
   7.69623833e-01  1.24234480e-01  5.18768993e-02  6.31172363e-01
   8.04681345e-03  2.50598019e-01 -6.66031088e-01  4.08751607e-01
   3.74789786e-01  3.47726911e-01 -4.95601654e-01 -2.90942234e-01
   2.33814064e-02  2.75110129e-01  1.75337820e-02  9.08067812e-02
  -1.99421352e-01  3.94568996e-02 -2.10284014e-01  1.15431995e-01
  -1.16363616e-01]
 [ 8.44658771e-02  1.31985039e-01 -4.64039908e-02 -2.47878469e-01
   4.39306472e-01  1.99143361e-01  2.00658719e-01  4.54484343e-01
   3.60056676e-01  6.73797894e-02 -1.97087800e-02 -2.59748106e-02
   8.86426485e-02 -2.63579833e-01  1.35649049e-01  1.62928819e-01
   1.78543921e-01  5.18188543e-01 -5.16399590e-01  1.36588358e-01
   4.17729672e-01  6.97172687e-01  3.84721546e-01  2.71994368e-01
  -1.29610870e-01]
 [ 1.23219385e-01  2.74083788e-01 -3.58422570e-01 -1.75333848e-01
  -3.41908926e-01 -6.78618293e-02 -6.31896430e-01  1.53648603e-02
  -5.91929143e-02  2.42423739e-01  5.97151391e-02  4.16330625e-01
  -1.42801723e-01  3.11415739e-01  3.83474417e-01 -1.00354173e-01
  -1.81350989e-01 -3.44907021e-01 -1.13442115e-01  1.16719064e-01
  -1.58489199e-01 -2.54097034e-01 -6.80185443e-01 -4.15273979e-01
  -3.87081080e-01]
 [-3.75652818e-02 -2.02500394e-01  2.00182219e-02  6.40364548e-02
   6.14268786e-02 -4.90589234e-02 -1.07125350e-01  3.10408730e-02
  -6.70738779e-02 -5.67929921e-01 -1.29790864e-01 -7.15156815e-02
   3.31424421e-02 -5.31873403e-01 -1.84070343e-01 -2.55564602e-01
  -2.95779656e-03 -5.63601967e-01 -1.25937566e-01  1.72774827e-01
  -5.97268611e-02 -3.31849540e-02  6.79411968e-02 -2.60956429e-01
   1.34332041e-01]
 [-4.52134708e-01 -5.01233933e-01 -1.59797246e-01 -3.92464244e-01
  -7.88105327e-02  7.74881744e-02  5.14294013e-01 -3.53942556e-01
   1.04057972e-01 -3.99354073e-01 -5.47833219e-01 -2.04398861e-01
   2.35870438e-01  1.45050474e-01 -9.41384568e-02 -1.13652741e-01
  -1.41511205e-01 -1.19864851e-01  2.57337532e-01 -1.02558248e-01
   3.05360298e-02  1.60856378e-01 -2.21179933e-01  3.25753512e-02
   4.02836867e-01]
 [ 3.44368729e-01  4.07978632e-01  1.15410117e-01 -3.11741678e-03
  -1.14062961e-01 -1.46648571e-01  1.57019909e-01 -3.26488626e-02
   5.49511325e-01  6.11574914e-01 -3.85457368e-01  6.61563891e-01
  -2.23211622e-01 -2.76565224e-01 -1.69347075e-01 -9.60782503e-02
   1.85491336e-01 -2.62279973e-02 -4.86791636e-02 -1.23842721e-01
   1.03319200e+00  9.10106002e-01  2.52751828e-03 -3.99932662e-01
   2.49515866e-01]
 [ 1.61075057e-01 -2.26281172e-01  2.29324186e-02 -1.29293473e-01
  -2.16448609e-01 -1.68943237e-01 -1.58830624e-01 -2.49807799e-01
   3.21566611e-02  1.74209863e-01 -1.22992631e-03  1.74862927e-01
   1.24026299e-01  1.64791534e-01  1.07458482e-01 -2.16532084e-01
   1.91824986e-01 -1.94523043e-02  5.01550617e-02 -2.87239293e-01
  -1.01293326e-01  1.63778940e-01  6.72970628e-01 -8.12557606e-02
   1.12961299e-01]
 [-7.21455099e-02  3.59808324e-01  1.80599457e-01  4.09448786e-01
   2.57104541e-02  7.58725762e-03 -3.80644659e-01 -1.45386780e-01
   6.82294715e-01 -3.64271312e-02  2.18975880e-01  3.71222638e-02
   2.04539260e-01 -9.70637066e-02  2.89673551e-01  8.68260489e-01
  -2.49649862e-02  2.96059278e-01  1.33744645e-01  2.55547001e-01
  -1.26009186e-01  2.85809437e-01  3.81929989e-01 -1.46912926e-01
  -1.12270381e-01]
 [ 1.19564273e-01 -5.66585622e-01 -4.31677731e-01  5.58521185e-02
  -5.61357977e-01  3.57664019e-02  1.81280943e-01  6.83127699e-01
   9.37820121e-02 -9.50645483e-02 -3.57826588e-01 -4.22128199e-02
   2.04606320e-01  4.14835317e-02  3.06788765e-01  2.09242843e-02
  -5.23581436e-01 -3.22334549e-01 -1.16262685e-01  2.13698051e-02
   5.32602334e-02 -3.06865292e-01 -3.12486378e-01 -3.53232762e-01
  -1.76825884e-01]
 [ 7.55001506e-01  3.32453876e-01 -2.28703338e-02 -3.54559188e-01
  -2.73217513e-01  4.52457066e-01 -2.38961674e-01 -5.69583159e-01
   1.35978450e-01  2.43637555e-01  1.21811820e-01 -2.19663999e-01
  -1.37449074e-01  1.35273805e-01 -8.88617267e-02  1.10920640e-01
   2.46919972e-01 -3.47886596e-02 -9.17040163e-02  2.41615472e-01
   1.62186078e-01  6.47185436e-02 -1.78071532e-01  4.82916083e-01
  -3.95264208e-01]] 
Matrix V:
 [[ 0.05657101  0.08695906 -0.36260902 ...  0.19368762  0.44816835
  -0.01653356]
 [ 0.35983988  0.18024505 -0.85865897 ... -0.20685778 -0.04159841
   0.15006403]
 [-0.31697213 -0.24777581 -0.25827145 ...  0.40765124 -0.17091622
   0.56513532]
 ...
 [ 0.20005586  0.62251618 -0.5678905  ...  0.40830477  0.42985087
   0.16502834]
 [-0.47833623 -0.31167075 -0.0727088  ...  0.14892793  0.2281662
   0.2404714 ]
 [-0.43023721  0.0980826   0.11279881 ... -0.30609456 -0.00994315
   0.13497076]] 
Matrix W:
 [[-0.45728619  0.26450605 -0.34722517 ...  0.20448287 -0.21260641
   0.3726617 ]
 [ 0.09371513 -0.32716312 -0.11457808 ... -0.70258505  0.26801527
  -0.10469297]
 [ 0.64914218  0.64889732  0.25436182 ...  0.03007868  0.75273396
   0.87913895]
 ...
 [ 0.29131586 -0.14996034  0.24997497 ...  0.58010543  0.01411488
  -0.05391826]
 [-0.36609923  0.09149286  0.18036051 ...  0.62616036  0.23847764
   0.1631463 ]
 [-0.10089582  0.32260801  0.02529173 ... -0.23229027 -0.30658861
   0.22524346]] 
===========================


Training model for 10 epochs
training set: 1000 sentences (batch size 100)
Optimizing loss on 1000 sentences
Vocab size: 2000
Hidden units: 25
Steps for back propagation: 5
Initial learning rate set to 0.1, annealing set to 5

calculating initial mean loss on dev set: 8.220088265486238

epoch 1, learning rate 0.1000	instance 1	epoch done in 49.06 seconds	new loss: 5.939081795999758
epoch 2, learning rate 0.0833	instance 1	epoch done in 49.04 seconds	new loss: 5.5541421635139185
epoch 3, learning rate 0.0714	instance 1	epoch done in 49.59 seconds	new loss: 5.451240552450753
epoch 4, learning rate 0.0625	instance 1	epoch done in 49.29 seconds	new loss: 5.388170133775631
epoch 5, learning rate 0.0556	instance 1	epoch done in 49.77 seconds	new loss: 5.341397484595132
epoch 6, learning rate 0.0500	instance 1	epoch done in 48.91 seconds	new loss: 5.305676083767191
epoch 7, learning rate 0.0455	instance 1	epoch done in 49.08 seconds	new loss: 5.276648710876227
epoch 8, learning rate 0.0417	instance 1	epoch done in 49.23 seconds	new loss: 5.250897739309841
epoch 9, learning rate 0.0385	instance 1	epoch done in 49.43 seconds	new loss: 5.228683813810095
epoch 10, learning rate 0.0357	instance 1	epoch done in 49.63 seconds	new loss: 5.209637875094277

training finished after reaching maximum of 10 epochs
best observed loss was 5.209637875094277, at epoch 10
setting U, V, W to matrices from best epoch

==========================
Matrix U:
 [[ 5.31336611e-01 -1.95054323e-01 -3.02524894e-01 -8.90631807e-02
  -6.29754244e-01  1.95294921e-01 -1.21668058e-01  3.42648779e-01
   1.94202832e-01 -6.02186195e-01  1.03087375e-03  4.57423152e-02
  -4.76937678e-01  6.65369742e-02  2.22209277e-01 -4.41075142e-01
  -4.37487914e-01 -3.07283249e-01  4.16853934e-01 -1.38796117e-01
   7.66250319e-02 -2.14985089e-01 -7.26672483e-01 -1.15749943e-01
   3.62329627e-01]
 [ 3.57812766e-01  2.20755777e-02  3.51743966e-01 -1.40740230e-02
  -1.06942340e-01 -9.09142707e-02  4.62546746e-01  1.93138950e-01
  -2.33448121e-01 -3.90457035e-02  4.63014956e-01  6.49173762e-01
  -3.87991579e-01 -4.77506820e-01 -1.27714177e-01 -8.31299329e-02
   3.74284080e-01 -8.46446172e-02 -4.53420535e-01 -5.45651641e-01
  -3.69265380e-01 -1.49784915e-01  6.96290043e-01 -1.98526999e-01
   3.77504560e-02]
 [-5.48750670e-01  3.89392266e-02 -4.48243162e-01 -8.08440545e-01
   5.74219423e-01  1.41737932e-01  2.66150516e-01 -1.83266542e-01
   5.79463598e-01  8.83312944e-02  1.04669451e-02 -2.99803872e-01
   1.72023746e-01  9.73976562e-01  1.21821132e-01 -4.56475416e-01
  -3.09773134e-01  4.56689842e-01 -4.09436943e-01 -3.48956192e-01
   2.97401334e-01 -3.70461703e-01 -1.03688251e-01 -4.77124712e-02
   3.71883981e-03]
 [ 1.23225423e-01  2.59588641e-01  2.63238969e-02  1.47206640e-01
   4.18708708e-01  1.20095828e-01  4.73742103e-01 -3.52351080e-01
   3.97674635e-01  1.11677991e-01  1.28997161e-01  4.80041112e-01
   3.85281144e-01  2.26177349e-01  4.29197812e-01  1.93388017e-01
   1.78222226e-01  2.90092940e-01 -4.24827047e-02  1.36479643e-01
  -5.43687147e-03  7.51170747e-01  1.67214472e-01  1.13630705e-01
   3.38326430e-01]
 [-1.62351507e-01 -6.83019078e-02  3.86260059e-01  1.93230987e-01
   2.51107988e-01 -2.82713686e-01  2.86360698e-01  2.16996566e-01
  -1.88756711e-01  4.02927105e-01 -1.83584638e-01  3.68265585e-01
   2.38553249e-01 -9.31163763e-03  1.33203557e-01 -4.97535911e-02
   5.62477522e-02  7.71566404e-01 -7.30880258e-01 -9.07574050e-02
  -8.50163416e-01  3.33897365e-01 -1.34831884e-02 -5.58970205e-01
  -4.15334288e-02]
 [ 5.86780303e-01 -3.53282936e-01  3.73822573e-01  3.94657405e-01
   3.15040223e-01  2.92939979e-01 -6.79938144e-02  4.83997604e-01
   2.77241108e-02  5.40512080e-01 -2.87489579e-01 -5.40421900e-02
  -4.98587542e-01  1.60649635e-01  1.26090421e-01  1.28839750e-01
  -5.32515635e-01 -3.88604068e-01 -1.64690543e-02 -9.36718649e-01
  -7.26123482e-02 -1.15903857e-01 -7.82642639e-01 -1.06308318e-01
  -6.50863403e-01]
 [ 1.26157817e-01  1.09148890e-02 -4.81000609e-01 -4.13691204e-02
  -8.27987990e-02  1.98573091e-01 -1.90794984e-01 -4.21404463e-01
   7.43449450e-02 -1.30153703e-01 -3.79780072e-01 -1.53577333e-01
  -7.94711359e-01  3.16126014e-01 -3.57027878e-01  2.99232220e-02
  -4.70394235e-03 -5.12279340e-01 -3.02888172e-01  2.04174599e-02
   7.46197342e-01 -1.37690031e-01 -9.15619447e-01 -4.40714075e-02
   2.95650174e-01]
 [ 1.37496055e-01  7.68731078e-02  2.01878819e-02 -4.27711210e-01
  -2.22607769e-01  2.85204375e-01 -4.62454221e-01 -4.11024341e-01
  -5.10229113e-01 -2.14660755e-01  5.14935058e-01  6.21787050e-02
  -2.14334063e-01  3.47295882e-01 -5.57428235e-01  1.45618867e-02
  -3.30158230e-01 -5.88068486e-02 -2.38055224e-01 -4.84258552e-01
   4.11334988e-01 -3.66061441e-01  1.29229221e-02 -1.92181815e-01
   7.00133946e-02]
 [ 4.08444766e-01 -4.42488265e-01  2.44227370e-01 -1.76796753e-01
  -4.42931551e-01  2.80462458e-02  6.94972284e-02 -4.81301415e-01
  -6.02704557e-01 -2.85863406e-02  4.06129176e-01 -3.61283265e-01
  -1.59884566e-01  9.13950152e-01 -1.06351642e-01 -1.97132031e-02
   1.69976668e-01  2.74081639e-01  3.90833207e-01  3.22219027e-01
   3.92302645e-01  1.75970121e-01  3.32199641e-01  1.22861552e-01
  -3.13188702e-01]
 [-3.46022047e-01  6.76065021e-02 -8.04221511e-02  1.60355163e-01
   1.96301101e-01 -1.37824704e-01 -2.47405878e-01  2.83282492e-02
   3.87335094e-01  5.92507726e-01 -1.53349532e-01  2.35324540e-01
   7.67669428e-01  3.61152637e-01 -5.17162027e-01  4.88754871e-03
  -3.13527978e-01 -1.91582955e-01 -8.20529715e-02 -8.22422177e-01
   4.44385234e-02 -5.12474007e-01  3.20476149e-01 -6.55889599e-01
  -2.77499794e-01]
 [-2.07216936e-01 -2.40510017e-01 -5.21698934e-01  2.13753411e-01
  -1.40501409e-01  1.75859349e-01 -9.42774014e-02 -1.62724982e-01
  -2.50442984e-01 -1.67279105e-01  3.27591775e-01 -8.65984127e-03
  -3.91036814e-01 -1.22659296e-01  3.80190411e-03 -3.46137536e-01
   5.35001540e-01  1.10950176e-01  3.22611686e-01  2.29929473e-01
  -2.70596566e-01  2.62887659e-02 -3.26332666e-01 -2.57554662e-01
   2.96416891e-03]
 [ 2.07794550e-02  4.36522514e-01  2.58370581e-02  4.28262028e-01
  -9.89936773e-02  2.78485990e-01 -1.84220297e-02  3.10588666e-01
   1.85533638e-01 -2.17499825e-01 -3.21189844e-01 -1.67262113e-02
  -1.81016007e-01 -6.45972126e-01 -5.30579234e-02  1.11634688e-01
   2.28806788e-01 -7.31759837e-02  7.61432081e-02 -2.41446309e-01
  -7.00009614e-01  2.05294339e-01 -4.59825039e-01  3.34944847e-01
   2.80316141e-01]
 [-2.32528126e-01 -3.18906668e-01  1.27110040e-01 -3.32606734e-01
   3.92186697e-01 -6.23701444e-02  5.33490883e-02  3.01087134e-01
   5.03170370e-01  4.91260507e-02  5.31893696e-01  3.00271356e-01
  -3.48083397e-02 -1.59159798e-01 -2.58562310e-01 -1.89039372e-01
  -1.46616230e-02 -3.40309403e-01  2.94096072e-02 -9.31579711e-03
  -1.63236731e-02 -6.69435551e-01  3.72112527e-01 -1.37896234e-01
  -2.76587117e-01]
 [-3.21444823e-01  2.87988685e-01  1.03097170e-01 -3.17686143e-01
  -3.58245339e-02  1.51342539e-01 -6.80886065e-03 -1.19053940e-01
  -3.04409373e-01  4.07126662e-01  1.73279617e-02 -6.06418860e-01
   7.29290199e-02  3.42676079e-01 -5.41018892e-01  6.89985265e-02
  -4.64804461e-01 -4.58334159e-01 -5.54392882e-01  2.92085492e-01
  -5.99610939e-01 -3.24803835e-01  2.14920346e-01  2.65311706e-01
   3.17454150e-01]
 [-4.78203757e-01 -8.40907480e-03 -1.63474139e-01  1.77017485e-01
  -2.63193562e-01  2.82151457e-01  5.64481122e-01 -7.53012651e-02
  -5.69183963e-01 -2.48815713e-01  4.39821121e-01  2.14290590e-01
  -1.40307175e-01  7.07243686e-01  1.09149194e-01 -3.40580992e-01
  -8.74050292e-02 -2.06455593e-01  3.33879507e-01  4.54295913e-01
   1.35645566e-01  2.81754471e-01  4.75339285e-01 -1.64135427e-02
   6.76798436e-02]
 [ 3.02554232e-01  1.72027791e-01  9.00022002e-02  7.40280401e-02
   5.89735058e-02 -4.09516410e-01 -1.96175696e-01 -3.16581726e-01
   7.76156056e-02 -4.12561743e-01 -2.50887135e-01  1.64381452e-01
   1.74539776e-01 -8.03509160e-04 -1.12855588e-01 -1.60321662e-01
   1.61003754e-01 -5.30554997e-02 -7.71568130e-02 -1.39551820e-01
  -1.13248595e-02  4.12538655e-02 -4.22576173e-01  6.02137714e-02
   5.37211986e-01]
 [ 3.21445230e-01 -2.11911876e-03 -2.59860696e-01 -8.71841145e-02
  -3.59539911e-01  1.01361223e-01 -1.81086951e-01  1.03127465e-01
   4.75709381e-01  4.46855066e-01  2.55451617e-01  3.63642295e-02
   5.11393572e-02 -2.95399631e-01  3.66148897e-01 -3.73133824e-02
   1.10113524e-01  4.58288481e-01  4.51926139e-01 -6.22992724e-01
   8.67842622e-02  4.43898267e-01 -1.85602641e-01 -4.45836425e-01
  -1.24246580e-01]
 [-1.99317771e-01  1.57313736e-01  3.06052267e-01  4.17848231e-01
   4.56650726e-01  7.34995735e-01 -6.59050898e-01 -4.70281190e-01
   2.34937492e-01  3.34380648e-02 -3.58757093e-01  1.18498184e-01
   5.35719435e-01 -4.71857284e-01  2.83584064e-01  5.44373918e-02
  -6.94510787e-02 -4.71953446e-01 -1.81766904e-01 -2.58492892e-01
  -4.36561182e-02 -3.33843109e-02 -4.76318923e-01  5.23428432e-01
   3.05034173e-02]
 [ 1.55828624e-01  5.06529872e-01  4.31247888e-01  9.29034339e-01
   2.61111743e-01  2.34757450e-01  2.15088998e-01  2.98462016e-01
   7.81411512e-01 -1.53208991e-01  2.99833738e-02  7.01184507e-02
  -4.49950698e-01 -4.02153121e-01  2.44196989e-01 -3.31628249e-01
   9.88509921e-04  1.72578633e-01 -1.16913501e-01  5.43335110e-01
   1.27771106e-01  4.13247305e-01  4.39002378e-01  4.60933462e-01
   1.20399113e-01]
 [-4.12898979e-01 -1.52457324e-01 -3.20443930e-02  1.89345773e-02
   2.38903717e-01  2.10319381e-01  3.00826118e-01  3.41939243e-01
  -7.87199637e-01 -2.60982923e-01 -3.53912327e-01 -3.09828862e-01
  -4.18386659e-01  2.04471892e-01  5.75217294e-03 -2.58661029e-01
   2.13634915e-01 -5.09465134e-01 -4.47062259e-01 -6.51872060e-02
   1.16733870e-01 -4.60868589e-02 -5.53437010e-01  1.42291072e-01
   1.42485727e-02]
 [-6.85535135e-02  2.51794109e-02 -2.47505591e-01 -8.24717066e-01
  -3.46649486e-01 -5.47003431e-02  3.12028702e-01  1.21849233e-01
  -5.76540971e-01  5.03980553e-01 -2.69153663e-01 -3.05579199e-01
  -2.05835121e-01  7.64603363e-02 -1.12819941e-01  5.22208474e-01
  -4.95005625e-01 -7.15820802e-02  2.10271171e-01 -2.97299361e-01
  -4.34088339e-01  1.02578012e-01 -4.82758945e-01 -4.99106969e-01
   5.31675816e-01]
 [-2.94470399e-01 -1.90184165e-01  2.36129221e-02 -2.96753267e-02
   1.35117309e-01 -3.19648454e-01  6.42995022e-02 -5.22856687e-01
   3.60800002e-01  4.18998076e-01 -1.73874730e-01  6.44496772e-02
   1.67094833e-01 -5.70452260e-01 -1.58515567e-01 -1.15015373e-01
   8.97623718e-02 -1.57171076e-01 -2.31885416e-01 -3.01198779e-01
  -3.76196642e-01  8.10386980e-01  3.07454097e-01 -1.17138498e-01
  -2.25576830e-01]
 [ 6.11186092e-01  4.15819720e-01  6.55188144e-01  1.69210075e-01
   6.26842670e-01  2.80634404e-02 -2.97744229e-01 -1.67093052e-01
   2.43403493e-01  1.15882004e-01  4.34519152e-01  5.68246409e-01
  -4.06740170e-01 -3.14553581e-01 -1.89096187e-01  6.81406148e-01
   5.10617232e-01 -2.51371953e-01 -1.68225872e-01 -4.98522540e-01
   1.99671892e-01 -4.09561516e-02 -6.26342031e-04  2.31583416e-01
  -2.31213092e-01]
 [ 2.08246726e-02  5.18825507e-01 -7.92465557e-02  6.52431288e-01
  -2.41265407e-01  2.02743122e-01 -4.21835339e-02  3.24209420e-01
  -1.43146253e-01  1.78371471e-01 -2.18023920e-01  3.44717305e-01
   2.71362400e-01 -1.30337432e-02 -5.53926632e-01 -1.24301311e-01
  -2.05044505e-01  4.45405563e-01  2.74721596e-01 -6.46678660e-02
  -4.09933906e-01 -3.30588986e-01  4.10474305e-01  3.08919799e-01
   1.93033271e-01]
 [ 5.15873198e-01 -1.13995262e-01  2.08275055e-01 -5.25572518e-02
  -1.83002057e-01  1.35798787e-01 -9.23656374e-02 -5.57749978e-01
  -4.79801499e-02  4.33011614e-02 -3.68688779e-01  4.90544739e-01
  -2.40160676e-02 -2.22739382e-01 -7.58625458e-01 -1.44169210e-01
  -4.23569010e-01  2.63680881e-02  7.86304538e-02  1.28169877e-01
   3.41006024e-01 -5.48750501e-01 -3.45193112e-01 -3.59651968e-01
  -2.02274706e-01]] 
Matrix V:
 [[-0.10996594 -0.38926546 -0.11181174 ... -0.56618294 -0.27195874
   0.22479951]
 [-0.05276516  0.44714989 -0.48629684 ... -0.05383675  0.08313678
   0.01364902]
 [-0.70211214  0.11979803  0.13752523 ... -0.5062629   0.28782361
   0.00585863]
 ...
 [-0.19360912  0.16476981 -0.37352613 ... -0.28937602 -0.19012636
  -0.06936614]
 [ 0.50267295 -0.24519213  0.45749239 ...  0.70922519  0.19854294
  -0.32902658]
 [ 0.1711775  -0.52013653  0.44601173 ... -0.45355918 -0.08772853
  -0.16902125]] 
Matrix W:
 [[ 0.08129386  0.59705003 -0.2700837  ...  0.65388593  0.65264019
   0.26944101]
 [-0.09310274 -0.13984611 -0.11166138 ...  0.05066198  0.17728097
   0.10446505]
 [ 0.15200507  0.58648626  0.04814599 ...  0.83204057  0.76717657
   0.43022408]
 ...
 [ 0.21186714  0.28460878  0.51964291 ...  0.16727824 -0.28305252
   0.23042072]
 [-0.01354664  0.06455789  0.31693153 ...  0.04925529 -0.28981015
  -0.37351138]
 [-0.26525404  0.0041472   0.14957603 ... -0.12435446 -0.24838787
   0.49607149]] 
===========================


Training model for 10 epochs
training set: 1000 sentences (batch size 100)
Optimizing loss on 1000 sentences
Vocab size: 2000
Hidden units: 50
Steps for back propagation: 0
Initial learning rate set to 0.1, annealing set to 5

calculating initial mean loss on dev set: 8.529788725926974

epoch 1, learning rate 0.1000	instance 1	epoch done in 40.73 seconds	new loss: 5.943425109943794
epoch 2, learning rate 0.0833	instance 1	epoch done in 41.02 seconds	new loss: 5.506663540728915
epoch 3, learning rate 0.0714	instance 1	epoch done in 40.74 seconds	new loss: 5.415174649084886
epoch 4, learning rate 0.0625	instance 1	epoch done in 41.23 seconds	new loss: 5.330311601945388
epoch 5, learning rate 0.0556	instance 1	epoch done in 40.66 seconds	new loss: 5.284552743098904
epoch 6, learning rate 0.0500	instance 1	epoch done in 40.74 seconds	new loss: 5.248513922885051
epoch 7, learning rate 0.0455	instance 1	epoch done in 40.67 seconds	new loss: 5.217101997505155
epoch 8, learning rate 0.0417	instance 1	epoch done in 41.04 seconds	new loss: 5.191705336598913
epoch 9, learning rate 0.0385	instance 1	epoch done in 64.30 seconds	new loss: 5.170826504895868
epoch 10, learning rate 0.0357	instance 1	epoch done in 43.91 seconds	new loss: 5.152268003240104

training finished after reaching maximum of 10 epochs
best observed loss was 5.152268003240104, at epoch 10
setting U, V, W to matrices from best epoch

==========================
Matrix U:
 [[ 1.14725908e-01 -7.96071188e-02  3.72882940e-01 ...  6.13865787e-02
  -2.48885204e-01 -2.61432435e-01]
 [ 1.85537137e-01  1.75404295e-01  1.25202091e-01 ... -1.91632946e-01
  -1.35959666e-01  1.01945549e-01]
 [-5.47849549e-03 -3.81389608e-01  3.56941833e-01 ... -4.12626820e-01
  -5.16367775e-01  4.09004172e-01]
 ...
 [-7.41829139e-01 -1.38137646e-01  2.25096844e-03 ... -3.48438160e-01
  -2.80117140e-01  4.51841444e-01]
 [-9.80560777e-02  2.68851840e-03 -1.35641447e-01 ... -2.22685467e-01
  -5.01813508e-01  6.96618117e-02]
 [-5.27175213e-01  1.59258207e-01 -1.76321972e-04 ...  5.69517703e-02
  -9.73615176e-02 -3.85575841e-01]] 
Matrix V:
 [[-9.93608274e-02  5.52283599e-01  5.19963693e-01 ... -4.66140693e-04
   8.20424902e-02 -8.39127764e-02]
 [ 1.61306178e-01 -4.75388790e-01  5.74209545e-01 ...  5.07461206e-02
   8.06174881e-02 -1.44370473e-01]
 [-4.58735059e-02  4.41041491e-02 -3.43291862e-01 ...  4.63677777e-01
  -6.77264016e-02  7.09983224e-01]
 ...
 [ 2.37721115e-01 -3.82260631e-01 -9.34366550e-02 ... -1.19212863e-01
   4.54164266e-02  2.08458657e-01]
 [-2.70134489e-01 -9.92909854e-02 -2.37616788e-01 ... -2.85626288e-01
  -3.31164381e-01 -1.62007722e-01]
 [ 5.09594646e-01 -2.67358362e-01 -1.07266937e-01 ...  1.25023018e-01
   2.29149225e-01  1.86566679e-01]] 
Matrix W:
 [[ 0.08676127 -0.10194653  0.26791076 ... -0.10552518  0.20527701
   0.39507434]
 [ 0.36210325  0.36391107 -0.13775217 ... -0.46150774 -0.0218385
  -0.12764089]
 [ 0.38369306 -0.22829554  0.15206136 ... -0.19681276  0.66236075
   0.30426481]
 ...
 [-0.03667239 -0.15318099  0.20752309 ...  0.46351247  0.46254554
  -0.11976991]
 [-0.21513716 -0.09491674  0.10653745 ...  0.07171145 -0.02785893
   0.26729283]
 [-0.32435964  0.41074537 -0.07492324 ...  0.68574597  0.10373623
   0.13337028]] 
===========================


Training model for 10 epochs
training set: 1000 sentences (batch size 100)
Optimizing loss on 1000 sentences
Vocab size: 2000
Hidden units: 50
Steps for back propagation: 2
Initial learning rate set to 0.1, annealing set to 5

calculating initial mean loss on dev set: 8.622490634921743

epoch 1, learning rate 0.1000	instance 1	epoch done in 51.33 seconds	new loss: 6.3190838847212145
epoch 2, learning rate 0.0833	instance 1	epoch done in 52.87 seconds	new loss: 5.4651961552558985
epoch 3, learning rate 0.0714	instance 1	epoch done in 50.75 seconds	new loss: 5.351179945648194
epoch 4, learning rate 0.0625	instance 1	epoch done in 50.54 seconds	new loss: 5.288592158274152
epoch 5, learning rate 0.0556	instance 1	epoch done in 51.03 seconds	new loss: 5.244431551949272
epoch 6, learning rate 0.0500	instance 1	epoch done in 51.14 seconds	new loss: 5.210745937843556
epoch 7, learning rate 0.0455	instance 1	epoch done in 51.57 seconds	new loss: 5.18419372253198
epoch 8, learning rate 0.0417	instance 1	epoch done in 51.19 seconds	new loss: 5.160121972763203
epoch 9, learning rate 0.0385	instance 1	epoch done in 50.11 seconds	new loss: 5.140233154322142
epoch 10, learning rate 0.0357	instance 1	epoch done in 50.67 seconds	new loss: 5.123840002858844

training finished after reaching maximum of 10 epochs
best observed loss was 5.123840002858844, at epoch 10
setting U, V, W to matrices from best epoch

==========================
Matrix U:
 [[-0.3708327  -0.61637567 -0.40213567 ... -0.14606029 -0.12338686
   0.44134945]
 [ 0.25681333  0.49584824 -0.01878129 ... -0.40802621  0.42597586
   0.87209513]
 [ 0.27836009  0.26520961 -0.10646863 ...  0.34848066 -0.26938993
  -0.22629897]
 ...
 [ 0.05066326  0.01935402  0.13420276 ... -0.00461691 -0.17133099
  -0.16374287]
 [-0.11691564 -0.04449459 -0.08537607 ... -0.0292727  -0.23848235
   0.29904135]
 [ 0.40916588  0.79726638  0.40161458 ...  0.20664405 -0.15098363
  -0.03665642]] 
Matrix V:
 [[ 0.06818426  0.22912903 -0.09467965 ...  0.48260676 -0.095938
  -0.08117184]
 [ 0.22414791  0.03018516  0.29687305 ... -0.11777802  0.05670191
  -0.42886094]
 [ 0.10963987  0.32828104 -0.41643923 ...  0.72599443 -0.08586818
   0.1156776 ]
 ...
 [ 0.02296496 -0.35098451  0.01827815 ... -0.05650739  0.14948351
  -0.09033597]
 [ 0.14285238  0.62745802  0.24850078 ...  0.30459276 -0.31670928
   0.139186  ]
 [ 0.1459844   0.17967093 -0.35945631 ... -0.1989753  -0.04856309
   0.1038067 ]] 
Matrix W:
 [[ 0.32145183  0.32663111  0.35873521 ...  0.09814018  0.14992378
   0.44576854]
 [ 0.18901745  0.07672676 -0.42027601 ...  0.6182436  -0.28875329
   0.16323407]
 [-0.06350916 -0.11484815 -0.06620265 ... -0.19943946  0.96360329
   0.43427246]
 ...
 [-0.30016602 -0.39424551  0.0046669  ...  0.03906002  0.24841141
  -0.11963999]
 [-0.11892917  0.45126086 -0.45948106 ...  0.07662964  0.02775771
  -0.21604196]
 [-0.49379328 -0.12078511 -0.13261215 ...  0.07059413 -0.04494071
  -0.20262649]] 
===========================


Training model for 10 epochs
training set: 1000 sentences (batch size 100)
Optimizing loss on 1000 sentences
Vocab size: 2000
Hidden units: 50
Steps for back propagation: 5
Initial learning rate set to 0.1, annealing set to 5

calculating initial mean loss on dev set: 8.713143605402111

epoch 1, learning rate 0.1000	instance 1	epoch done in 65.01 seconds	new loss: 5.707490810721253
epoch 2, learning rate 0.0833	instance 1	epoch done in 65.17 seconds	new loss: 5.482650340000934
epoch 3, learning rate 0.0714	instance 1	epoch done in 64.98 seconds	new loss: 5.382057433089497
epoch 4, learning rate 0.0625	instance 1	epoch done in 66.06 seconds	new loss: 5.319984381195006
epoch 5, learning rate 0.0556	instance 1	epoch done in 65.46 seconds	new loss: 5.2725980526192
epoch 6, learning rate 0.0500	instance 1	epoch done in 65.36 seconds	new loss: 5.235928576976777
epoch 7, learning rate 0.0455	instance 1	epoch done in 65.43 seconds	new loss: 5.205101815166097
epoch 8, learning rate 0.0417	instance 1	epoch done in 65.35 seconds	new loss: 5.180857013538234
epoch 9, learning rate 0.0385	instance 1	epoch done in 66.17 seconds	new loss: 5.159369665318631
epoch 10, learning rate 0.0357	instance 1	epoch done in 65.26 seconds	new loss: 5.140867883246269

training finished after reaching maximum of 10 epochs
best observed loss was 5.140867883246269, at epoch 10
setting U, V, W to matrices from best epoch

==========================
Matrix U:
 [[ 0.40762978 -0.64767738 -0.59673433 ... -0.41433581  0.00793446
   0.34467935]
 [ 0.21719587  0.2130172  -0.25600942 ... -0.10987783 -0.42771673
  -0.13349949]
 [ 0.33537728 -0.6134973   0.22678999 ...  0.18872467 -0.21982973
   0.03528164]
 ...
 [ 0.14477068 -0.47866696 -0.21794598 ...  0.04191738  0.10753348
   0.1949412 ]
 [-0.14119662 -0.11817108 -0.38703675 ... -0.52300943  0.18719544
  -0.12907474]
 [-0.4936947  -0.34089912  0.12577229 ...  0.28379929  0.12122387
   0.14044717]] 
Matrix V:
 [[ 0.17016399 -0.01378297  0.06843203 ...  0.15421295  0.4521834
  -0.28962669]
 [-0.2763595  -0.34800452 -0.32239643 ...  0.15470737 -0.03452219
  -0.00174476]
 [ 0.34943769 -0.07705918  0.88273347 ...  0.38993187  0.52763155
   0.31654616]
 ...
 [ 0.26068807 -0.31382648 -0.69718855 ... -0.02529903  0.05803548
   0.03103537]
 [-0.25656257 -0.56261895 -0.04637868 ...  0.24595978  0.48746745
  -0.23308247]
 [ 0.48127752 -0.27079122 -0.23358858 ...  0.56176689  0.01599187
  -0.35192324]] 
Matrix W:
 [[ 0.02873858 -0.43998234  0.483861   ...  0.48614537  0.30049535
   0.47608125]
 [-0.1560143   0.50960573 -0.73067246 ...  0.4068772   0.17315909
  -0.48370066]
 [-0.25182311  0.41570607 -0.06584246 ...  0.81027172  0.06816281
   0.00704406]
 ...
 [ 0.0906597  -0.03524009  0.26007124 ... -0.14042816  0.04888425
   0.23666004]
 [ 0.51065085 -0.52922797 -0.07924376 ... -0.10554807  0.48910834
  -0.1289105 ]
 [ 0.0699511  -0.33469656  0.54778889 ... -0.03264632  0.41560245
   0.25731445]] 
===========================


Training model for 10 epochs
training set: 1000 sentences (batch size 100)
Optimizing loss on 1000 sentences
Vocab size: 2000
Hidden units: 25
Steps for back propagation: 0
Initial learning rate set to 0.05, annealing set to 5

calculating initial mean loss on dev set: 7.7982082154310515

epoch 1, learning rate 0.0500	instance 1	epoch done in 35.69 seconds	new loss: 6.238352371786258
epoch 2, learning rate 0.0417	instance 1	epoch done in 35.05 seconds	new loss: 5.77518259410392
epoch 3, learning rate 0.0357	instance 1	epoch done in 35.29 seconds	new loss: 5.594344547977013
epoch 4, learning rate 0.0312	instance 1	epoch done in 35.44 seconds	new loss: 5.518851211964928
epoch 5, learning rate 0.0278	instance 1	epoch done in 35.00 seconds	new loss: 5.474506022992276
epoch 6, learning rate 0.0250	instance 1	epoch done in 35.62 seconds	new loss: 5.441935736870711
epoch 7, learning rate 0.0227	instance 1	epoch done in 35.40 seconds	new loss: 5.415606902874582
epoch 8, learning rate 0.0208	instance 1	epoch done in 35.48 seconds	new loss: 5.3940244813488025
epoch 9, learning rate 0.0192	instance 1	epoch done in 35.80 seconds	new loss: 5.375811488598643
epoch 10, learning rate 0.0179	instance 1	epoch done in 35.01 seconds	new loss: 5.3594378954376625

training finished after reaching maximum of 10 epochs
best observed loss was 5.3594378954376625, at epoch 10
setting U, V, W to matrices from best epoch

==========================
Matrix U:
 [[ 3.46841476e-02 -1.20867244e-02  2.12005314e-02 -1.04652534e-01
   2.37179516e-01 -1.78843797e-01  2.91716042e-01  5.71932415e-01
   4.83398398e-01  2.38763712e-01  1.61270377e-01 -6.64901694e-02
   1.04264411e-01 -1.13484475e-01 -1.08593858e-01  4.28532897e-01
  -3.40086482e-01 -1.79926835e-01 -8.19843792e-02  7.65375721e-02
  -3.26306422e-01  5.78776760e-02  2.24785979e-01 -4.77394283e-01
  -1.10981273e-01]
 [ 1.53675497e-02  2.03521924e-01 -1.05579516e-01  3.23578532e-01
   7.56616567e-02 -5.63789411e-01  5.23337275e-02 -1.34358212e-01
   2.19362470e-01  5.58132664e-02  3.99750003e-01  3.03598137e-01
  -5.05745102e-01  9.02921680e-02  1.21530474e-01  1.43298394e-02
   3.06805258e-01  3.32236576e-01  1.06504124e-01  3.26115904e-01
   6.18715096e-02  4.22993272e-01  2.62213168e-01  2.92184650e-01
  -9.07969782e-02]
 [-2.54624688e-01  5.34628543e-02 -1.93453488e-01 -6.99282202e-02
  -3.73791722e-01  6.91435949e-01  1.02515623e-01 -1.30765081e-01
   1.42380101e-01 -4.62025127e-01 -3.95024998e-01 -1.64912121e-01
  -1.58047558e-01 -1.72251452e-01  2.15108548e-01 -6.67164946e-02
  -2.71082838e-02 -2.08757202e-02 -2.36763272e-02 -8.24614196e-02
   4.96881174e-03  3.51253424e-01  1.71347169e-01 -4.68161641e-01
   5.50071118e-02]
 [ 3.73879443e-02 -4.80099137e-01  3.00223429e-02  7.23198916e-02
   2.95459586e-01 -1.00813985e-01 -4.49251505e-02 -4.70537491e-02
   6.00690706e-02  1.40127051e-01  1.76325660e-01  6.38975670e-02
  -2.38240305e-01  1.38162314e-01  1.75427842e-01  1.11642516e-03
  -8.04472379e-01 -1.86575436e-01 -1.11029815e-01  5.47691864e-01
   2.48219119e-01  2.79007265e-01  4.62903004e-01 -2.26558917e-01
  -1.25054301e-01]
 [-3.29658670e-01  3.65658094e-01 -3.65182677e-01  2.09365095e-01
   1.57593025e-02 -2.84579958e-02  1.88075014e-01  5.55035485e-02
  -6.91248703e-01 -3.02464922e-01  4.57173029e-01  6.67066895e-02
  -5.38908090e-01  6.87162482e-01  3.50210870e-01  2.95247819e-01
   2.57710201e-01  2.00591188e-02  2.25473313e-01  1.65290567e-01
  -1.51134146e-01  4.34552246e-01 -4.67082322e-01  1.75473569e-01
   8.32699813e-02]
 [-3.73437562e-01 -1.73395928e-01 -2.28848529e-01 -6.27772580e-01
  -1.04672284e-01  4.30929589e-01 -4.33368373e-01  5.93931877e-02
   2.79120434e-01  5.33850851e-01  2.65497999e-01  9.98086313e-02
  -3.10076071e-01 -7.99922000e-02  1.78078485e-01 -2.62895932e-01
  -7.24934101e-01 -1.35060767e-02 -1.16948645e-01  2.34078906e-01
   7.88183839e-02  5.17154632e-02  3.42712176e-01 -1.42692004e-01
  -1.76661928e-01]
 [-4.01859810e-01  6.50125105e-02 -5.14349548e-01  2.28135221e-01
   1.18869273e-01  7.32793963e-01  1.48881707e-01 -3.24619741e-01
  -9.13631633e-02 -3.35700081e-02  3.74991144e-01 -2.17549854e-01
   5.56281726e-02 -1.96346558e-01  4.49631977e-01  3.30137963e-01
  -1.36597211e-01 -1.90441458e-01 -2.11564114e-01  1.81358614e-01
   4.18725717e-01 -2.00972382e-01  3.50406291e-01 -4.07258128e-03
   2.13842503e-01]
 [ 3.68640528e-01 -7.06439006e-02  5.46811187e-01  1.35126847e-01
   2.58866939e-01  3.92584120e-01  5.07683766e-03  3.41169351e-02
   2.77798030e-01  4.32169570e-01 -3.02737602e-01  8.86369002e-01
   5.11677309e-01  5.22282839e-01  2.03276223e-01 -6.07890241e-02
   3.58787771e-02 -4.61310764e-01  5.85165459e-01 -5.09805153e-01
   8.93796232e-02  6.27244705e-01 -4.45690562e-01 -2.27005472e-01
   2.11936220e-01]
 [ 4.64954802e-01 -3.44156324e-01 -3.27062087e-01 -4.12456610e-01
   7.36284519e-03 -2.75133870e-01 -2.89970848e-01 -6.57686784e-01
  -9.46983380e-02  9.80360091e-02 -2.14101883e-01  3.54538207e-01
  -1.76077985e-01  3.81229975e-01  3.92484215e-01 -5.45095784e-01
   1.96591771e-01  9.84037093e-02 -4.56796102e-02 -4.72538963e-01
   1.39869303e-01  2.69382166e-01 -1.11347899e-01 -3.63801718e-02
   2.17192051e-01]
 [-8.47769623e-02  2.00651480e-01  3.36793647e-01 -2.63227066e-01
   1.20774521e-01 -4.08275781e-02  1.03854625e-01 -2.20890153e-01
  -2.51586780e-01 -2.65955865e-01 -1.71145588e-01 -2.22916734e-01
  -4.39193533e-01 -2.47609235e-01 -6.35139592e-01 -9.67762086e-01
   1.68672179e-01  3.81245911e-01  5.03894910e-01  1.67319784e-01
   2.60347019e-01 -2.69292010e-01 -1.94987870e-01 -4.89698225e-02
   8.74288988e-02]
 [-5.51153217e-02  3.02069837e-01  2.44846872e-01  5.59237790e-02
  -4.67216866e-01 -2.99254341e-01 -1.24089632e-01  5.13866749e-01
  -5.05746450e-01 -8.94492349e-02  6.34019319e-03 -1.82414174e-01
   1.75886573e-01  7.94779985e-02 -6.43386893e-01  7.42947264e-02
   3.60723736e-02 -1.38052387e-02 -2.97560569e-01 -5.30476736e-01
  -6.57871957e-01  5.76507268e-01 -2.07798657e-02  6.10333597e-02
   2.65222791e-01]
 [ 2.03358204e-01 -2.26394874e-01 -1.84187446e-01  3.51922353e-02
   1.83519661e-01  1.20663815e-02  1.17906531e-01  1.07917780e-02
  -1.12205183e-01 -4.72288778e-01 -2.53920811e-01  8.44477321e-02
   5.50537007e-01  3.72955995e-01 -2.94197059e-01 -5.98715052e-03
   3.52829530e-01 -1.05039118e-01 -1.82889170e-01 -1.24695593e-01
  -2.32036969e-01 -4.67843194e-01  1.25683131e-01  3.36288312e-01
   4.44555955e-01]
 [ 2.64863077e-01  4.10853424e-01 -1.27178862e-01  1.48016302e-01
   5.04137619e-02 -1.47900484e-01 -6.29199210e-01  4.38778269e-02
  -3.24801983e-01  2.47974562e-01  2.16835757e-01  2.43055756e-04
   2.94003047e-01  8.60969624e-03  3.36959589e-01  2.46789613e-01
  -4.88319077e-02 -6.03034891e-01  4.01857359e-01 -1.63265601e-01
  -8.51396561e-02  2.98866659e-01  9.48057108e-02  1.40128190e-01
   1.40941919e-01]
 [-2.11618835e-01  1.31765440e-01  2.61250860e-01  3.39521638e-01
  -4.46551263e-02 -8.61915078e-02  2.46336160e-01  4.35647737e-01
  -4.00283538e-01 -1.48892864e-02  3.41518300e-01  4.60748805e-01
   3.75372778e-01  3.11530947e-01 -1.21665037e-01 -1.04388372e-01
   6.32194616e-02  6.02886864e-02 -3.66628123e-01  3.62987558e-01
  -3.55299668e-01  3.94345480e-01  8.46062097e-02 -5.75705780e-02
   5.92789409e-01]
 [ 3.47110417e-01 -1.14302960e-01 -4.48576226e-01  4.93790374e-01
   1.29819574e-01  1.06484849e-01  1.97155520e-01  2.12030401e-01
  -6.07233008e-01  1.38322741e-01 -5.39749386e-01  8.77982149e-02
   5.24103643e-01  4.03239557e-01  3.37899536e-01 -4.43215622e-02
  -2.59746639e-01 -6.48102513e-03  7.25950599e-02 -3.25026472e-01
  -2.01593100e-01  3.45094557e-02 -9.15292245e-02 -3.10374601e-01
  -3.64101151e-02]
 [ 6.40061093e-01  4.11960311e-01  7.60209022e-01 -2.47980149e-01
   4.34924004e-01  2.84733920e-01  8.62752981e-01  1.77165994e-01
  -1.83878211e-01  1.09533068e-01  2.21075939e-01  4.50611440e-01
   1.79278404e-01 -2.40308132e-01  1.30477669e-01  2.35644252e-01
   4.07647925e-01  1.01229769e-01  4.92515328e-01  2.39457593e-01
  -1.85931795e-01  3.36429643e-01 -1.00409240e-01  3.45053124e-01
  -3.98550735e-01]
 [ 5.94743634e-02 -3.18124360e-01 -1.24724403e-01 -2.62301820e-01
  -4.60336504e-01 -5.01859017e-01 -6.93647333e-02 -5.44410887e-01
  -3.65229933e-01 -2.04618471e-01 -7.99296766e-03  2.87614602e-01
  -2.07459098e-01 -1.90691494e-01  1.27104245e-01  5.16567460e-01
  -1.78095283e-01 -7.81313344e-02  1.73932207e-01 -1.41935733e-01
  -1.54511545e-01  7.34970376e-02  4.19495936e-01 -3.88868956e-01
   2.13981629e-01]
 [-2.38590038e-01  1.99333292e-01  5.86469701e-02 -6.90698311e-02
   1.85736912e-01 -9.73668992e-01  4.41227516e-01  4.05808200e-01
  -3.55901387e-01  3.06963293e-01 -1.65744182e-01 -4.79479223e-01
  -6.49223289e-03 -1.07591357e-01  4.72302012e-01 -3.09594069e-01
   5.66031274e-02  2.99917534e-01 -3.57398190e-01  4.25341781e-01
  -2.70246684e-01 -2.44822149e-02  1.67215455e-01  2.91309257e-01
  -2.18372024e-01]
 [ 1.18275139e-01 -3.76392447e-01 -2.25317308e-01  1.33213949e-01
   3.71758698e-01 -1.72375950e-01 -5.82860238e-01  1.60426008e-01
   9.24114423e-02  6.72393731e-02  3.28527296e-02  2.19290872e-01
  -2.16703022e-01 -8.06424629e-01 -9.62728985e-03 -2.49061064e-01
   1.81896084e-01  2.58843006e-01  1.26523734e-02 -1.88396766e-02
   3.35225908e-01  1.66386411e-01  4.12643351e-01 -5.01621022e-01
   1.26894127e-01]
 [ 2.38556037e-01 -2.14631206e-01 -1.63962708e-01 -2.17962114e-01
  -8.99855159e-01  2.61203525e-01  6.12695482e-02  2.15754275e-01
   1.77387824e-01 -2.73131709e-01 -2.10437027e-02  1.02428163e-02
   6.20714880e-01  1.19227112e-01 -3.80606633e-01  2.70017976e-01
   9.63116796e-01 -1.91863485e-01  2.43551891e-01 -2.35707240e-01
   1.40017507e-01 -1.41410604e-01 -5.67610773e-02  3.67950123e-01
   1.32672323e-01]
 [ 1.62861086e-01 -3.19613907e-01  2.33156610e-01 -4.49588647e-01
   3.93415719e-02  1.61841956e-01 -1.43324215e-01  3.47489939e-01
   3.29073882e-01  1.97027296e-01  2.68557597e-01 -3.39833040e-01
   1.21359006e-02  1.65198707e-01 -3.27475167e-01  1.58655866e-01
   2.94480335e-01 -3.67601558e-03  5.74002422e-02 -2.48136170e-01
   1.00850690e-01 -3.51312282e-01  3.54065549e-01  7.11749746e-02
  -3.98343501e-01]
 [ 3.07422185e-01 -4.37571453e-01 -2.55311666e-01 -3.80514029e-02
   1.52632192e-01  9.08799619e-02 -3.57818866e-01  7.56313428e-01
   1.31829114e-01 -6.27005631e-03  9.42214588e-04  1.71119120e-01
  -1.60235679e-01  4.47009534e-01  2.86711498e-02 -2.05095188e-01
  -2.47863766e-01  1.09712081e-02 -9.13229825e-02 -4.18315194e-01
  -3.83706352e-01 -2.09803133e-01 -6.85614341e-02  3.21527147e-01
  -1.32075047e-01]
 [-3.08861297e-01  1.76921807e-02 -1.30423276e-01 -1.47860085e-01
   8.12625063e-02  8.68751837e-02 -2.70844671e-01  1.68274133e-01
   4.27819753e-01  3.88622200e-01  2.80989792e-01 -6.12591778e-02
  -3.56599505e-01  9.04632428e-03 -1.69313365e-01  2.70034366e-01
  -6.77524940e-01 -6.63062218e-02  8.74578573e-02  1.38768137e-01
   8.46433668e-01  1.92027211e-01 -8.61302720e-02  3.70434203e-02
   6.44222971e-01]
 [-7.46808087e-01 -6.93185300e-01 -3.17181345e-01  8.94240119e-02
   2.93955242e-01  5.55683701e-03  4.73312304e-01 -1.57283101e-01
  -3.34035147e-01  1.19039350e-01 -7.79502616e-02  1.89365624e-01
  -3.71653478e-01 -2.07186600e-01  5.22202424e-01  4.08575921e-01
   3.08402937e-02  3.83482474e-01  4.08301828e-01  2.22870334e-01
   2.61053782e-01  5.44845863e-01 -3.18035354e-01 -1.68834194e-01
   4.70617610e-01]
 [-1.58211778e-01 -2.26260513e-02  2.47250847e-01  3.41913302e-01
   4.86694486e-01 -7.12067989e-01  1.16148418e-01  5.03972448e-02
  -2.60543087e-01 -4.93123298e-02 -3.53147088e-02 -2.49726130e-01
   9.87889667e-02 -7.61442594e-01 -4.59998768e-01 -4.52581601e-01
  -6.74994821e-02 -4.76149352e-01  6.98845824e-02 -3.85925912e-01
  -2.12986764e-03 -2.77619002e-01  3.83997174e-01  1.56667557e-01
  -8.26865845e-01]] 
Matrix V:
 [[ 0.17746562 -0.14454184  0.24577639 ... -0.27329237 -0.01750753
  -0.13865412]
 [ 0.05559231 -0.10688401 -0.28375358 ...  0.14370585 -0.36900621
  -0.31242492]
 [ 0.13529379 -0.48041531 -0.29505635 ...  0.28354137 -0.04962456
   0.25509654]
 ...
 [ 0.03789076  0.5154319  -0.27759317 ...  0.18991347 -0.094411
   0.33566181]
 [ 0.04960498 -0.31266967 -0.23042963 ...  0.23017131  0.24660044
  -0.05170282]
 [ 0.18993709 -0.05496851 -0.04842624 ... -0.39574596  0.07091958
   0.06196654]] 
Matrix W:
 [[ 0.22628244  0.48301737  0.02185266 ...  0.1731446  -0.01714827
  -0.36752355]
 [-0.2091722  -0.22378868  0.01720384 ...  0.33436093 -0.14582788
   0.23306674]
 [ 1.03817565  0.7055255   0.08347988 ...  0.57730559  0.83337299
   0.55682476]
 ...
 [-0.31918626  0.16821177  0.34564705 ...  0.03766964 -0.10620003
   0.39832762]
 [-0.68884789  0.76019926 -0.28324722 ... -0.48848745  0.32396625
   0.15537613]
 [ 0.19009262  0.05012038  0.0787751  ...  0.39775241 -0.0878339
  -0.13054472]] 
===========================


Training model for 10 epochs
training set: 1000 sentences (batch size 100)
Optimizing loss on 1000 sentences
Vocab size: 2000
Hidden units: 25
Steps for back propagation: 2
Initial learning rate set to 0.05, annealing set to 5

calculating initial mean loss on dev set: 8.173202780610817

epoch 1, learning rate 0.0500	instance 1	epoch done in 41.41 seconds	new loss: 6.2034452749070725
epoch 2, learning rate 0.0417	instance 1	epoch done in 40.62 seconds	new loss: 5.740010110110972
epoch 3, learning rate 0.0357	instance 1	epoch done in 40.96 seconds	new loss: 5.610302424133665
epoch 4, learning rate 0.0312	instance 1	epoch done in 40.93 seconds	new loss: 5.544934797310444
epoch 5, learning rate 0.0278	instance 1	epoch done in 41.08 seconds	new loss: 5.499441691716861
epoch 6, learning rate 0.0250	instance 1	epoch done in 41.13 seconds	new loss: 5.465574755578675
epoch 7, learning rate 0.0227	instance 1	epoch done in 41.33 seconds	new loss: 5.439091637657618
epoch 8, learning rate 0.0208	instance 1	epoch done in 40.89 seconds	new loss: 5.416274191456299
epoch 9, learning rate 0.0192	instance 1	epoch done in 41.38 seconds	new loss: 5.3974408394120275
epoch 10, learning rate 0.0179	instance 1	epoch done in 40.72 seconds	new loss: 5.3808138917272945

training finished after reaching maximum of 10 epochs
best observed loss was 5.3808138917272945, at epoch 10
setting U, V, W to matrices from best epoch

==========================
Matrix U:
 [[ 3.23857890e-01  1.35616327e-01 -5.27020031e-02 -3.49564963e-01
   4.21438310e-01  9.03187815e-02  2.56260011e-02 -8.61770938e-02
   8.13248102e-01  2.73537486e-01 -2.83753442e-01 -1.41891848e-01
   8.98927176e-02  1.42717229e-01 -1.18603926e-01 -4.12131778e-01
  -1.98194061e-01 -2.26102499e-01 -4.09602806e-03  5.40750703e-01
  -6.64833674e-01 -2.82628256e-01  5.51483340e-02  7.22637349e-01
   6.14526509e-02]
 [-3.05778592e-01  2.54243431e-01  1.36182368e-02  5.07551524e-01
   2.51370811e-02 -1.40177029e-01  3.02660670e-03  3.06804234e-02
   2.92823721e-01  3.64714639e-01 -1.00643238e-01  1.45059258e-01
   1.62821372e-01 -7.72459590e-01 -1.66537671e-01  5.28819953e-01
   1.14410144e-01  2.53077029e-01 -2.79718207e-01  3.33406271e-01
   1.80342841e-01  1.31816803e-01 -2.35287448e-01 -8.01759276e-02
  -1.98988638e-01]
 [ 3.08474728e-01 -4.84678385e-01 -3.71559693e-02  5.57928628e-02
   3.41765585e-01  7.83840534e-04  3.74820612e-02  3.70395681e-02
  -7.20211740e-01  2.21110404e-01 -1.53942899e-01 -4.74494443e-01
  -2.09089220e-02 -1.85022802e-01  3.64704990e-01 -3.47116591e-01
   1.82366672e-01 -4.65736053e-01 -4.34166112e-01 -4.23993842e-01
  -3.61973046e-01  2.64742472e-01 -2.36183046e-01 -1.38126789e-01
   1.50366324e-01]
 [-3.01588534e-02  4.34220038e-01  2.53285838e-01  3.91088931e-01
   1.06477652e-01  2.43897168e-02  1.61674987e-02  1.27139900e-01
  -3.73608679e-01 -1.64601185e-01 -4.33720600e-03  1.88548857e-01
  -9.49400100e-02  1.73741059e-01  1.99037993e-01  2.67181644e-01
   1.49125268e-02  5.50647787e-01 -1.75497741e-01 -2.77839614e-01
  -8.57265711e-02 -4.63351089e-02  3.49590737e-01 -5.00852238e-01
  -8.62952603e-02]
 [-6.42441189e-01  3.56320852e-01  2.04321993e-01  2.43236487e-01
   3.05046643e-01  1.14363208e-01  6.59028616e-02 -1.01420906e-01
  -3.12526991e-01  1.04651361e-01 -5.81674646e-03  2.95834658e-01
  -2.55814771e-01  7.06335254e-03 -2.45904892e-01 -1.38254311e-01
  -2.54685725e-01 -1.16512201e-01  2.15712399e-01  4.55459561e-01
   1.27270764e-01 -3.63806797e-01 -5.63497603e-02 -8.62085223e-01
   2.05859936e-01]
 [ 4.19686321e-01  1.31559141e-01 -3.54353801e-01 -2.43370881e-02
  -1.05639389e-01 -4.39787960e-01 -2.21026220e-01  3.35649294e-01
   1.53759393e-01  5.77209721e-03 -1.01780285e-01 -4.41799706e-01
  -1.62499060e-01  2.26693341e-01  2.90596532e-01  2.61514987e-01
   4.61667806e-01  1.21764257e-01 -2.28737573e-01 -2.78669348e-01
  -1.66105761e-02  6.27674114e-02 -1.43259145e-01 -4.90408888e-01
   3.32177887e-01]
 [-9.20151329e-02  3.59207020e-01  1.96245463e-01 -1.60883660e-01
   1.38318082e-01 -1.10251948e-01 -1.07796311e-01 -2.48066635e-01
   1.00807948e-01  5.79962877e-01  1.94477964e-01 -3.29675419e-01
   2.36346401e-02  5.30743477e-01 -4.31730678e-01 -4.27173746e-02
   2.54356962e-01 -2.15833760e-01  4.22993161e-02 -1.58097027e-01
   4.45208586e-01 -1.36991659e-01  1.46681245e-01  2.21782699e-01
  -5.40862510e-01]
 [-3.84726045e-02  3.47380007e-01 -1.80410919e-01 -7.22949520e-01
  -2.52692528e-01  3.63684917e-01  5.73320304e-01  1.70757436e-01
  -2.25037366e-01 -1.17001306e-01  1.57187768e-01  1.09135980e-01
  -6.47895907e-01 -2.03608540e-01 -2.32505267e-01 -2.86349983e-01
   2.48408891e-01  8.16390872e-01 -3.94796720e-01 -5.48733508e-01
  -7.49147739e-02 -4.02601021e-01 -4.92922450e-01 -2.59953114e-01
  -1.47814029e-01]
 [ 2.10124985e-01  4.59560272e-03  1.56879186e-01  5.01302145e-02
  -1.02747877e-01 -6.58279466e-02  2.09676148e-02 -5.20586150e-01
   4.74674436e-01 -1.02402221e-01  1.01654882e-01  1.78657633e-01
  -6.86321092e-03 -6.93755021e-03  5.83943677e-02  3.18910017e-02
   8.59989342e-02  2.60932950e-01  1.09208399e-01  2.97232349e-01
   4.03097373e-01  5.17215498e-01  1.06635980e-01 -2.99081083e-01
  -2.20661595e-02]
 [ 4.76326683e-01  1.86893022e-01  1.29354742e-01 -2.17954271e-01
   3.83555604e-01 -1.34019314e-01  4.71673704e-03 -1.29821756e-01
   2.68107030e-01 -2.03432069e-01 -1.44697075e-01 -5.12181427e-01
   6.72450484e-03 -2.34371141e-02 -8.74009355e-02  5.66891708e-01
  -5.29401120e-03 -8.71840134e-01  1.86565798e-01  3.21197350e-01
   1.11863222e-01  5.39214898e-01 -5.03333585e-02  1.34087692e-01
   4.07628823e-01]
 [-4.38100071e-01 -2.89341035e-01  1.09430635e-01 -5.46593670e-01
  -1.62838893e-01  4.71240211e-01  4.96205621e-01 -3.55160864e-01
  -1.75113117e-01  1.40767442e-01 -4.77214379e-01  9.66034268e-02
   1.60888884e-01  1.19344354e-01  9.80746734e-02  2.33209314e-01
   2.54582684e-01 -6.01263951e-01 -6.39904615e-01 -7.27318015e-01
  -7.98804895e-02 -3.99686087e-02 -1.28781954e-01  1.66867161e-02
   3.45468682e-01]
 [-1.28852077e-01  3.54916890e-01  2.60856718e-01  2.48244110e-01
  -7.82614953e-02 -4.70064180e-01  9.72334644e-02 -2.34556871e-01
   2.74608624e-02  4.52121293e-01 -2.33056695e-02  1.21867060e-02
   6.15490338e-01  4.95696276e-01  1.61885728e-01 -1.04456945e-01
   7.14349493e-02 -2.28226407e-01  9.21149049e-02  7.93831270e-02
   4.06386146e-01  1.86386796e-01  4.19825660e-01  4.84546007e-01
   6.15129512e-02]
 [ 1.87928243e-01  3.33167830e-01 -2.51553271e-01 -6.74342904e-02
   4.33401850e-01  3.12658069e-01 -2.31566838e-01  3.49535424e-01
  -4.81828556e-02  3.02453457e-01  2.42899222e-01  5.65687995e-01
   3.72355075e-01  6.18833209e-01 -3.76487266e-02  2.79915524e-01
  -2.26253387e-05  1.43539523e-01  1.10703371e-01  2.79266128e-01
   5.39393915e-02 -2.75097177e-01  1.15047879e-01  1.11051501e-01
   3.22634474e-01]
 [-1.47374450e-01  3.84613085e-01  1.39068242e-01 -2.42756107e-01
   3.27795529e-01 -4.41319479e-01 -4.55067548e-01  3.94921421e-01
  -1.19581483e-01  1.27874875e-01 -1.39990185e-01 -2.84779462e-01
   2.76888060e-01 -1.79711727e-02  2.33800295e-01  7.84793362e-02
  -2.11990112e-01  4.64887629e-02  5.15443710e-01  5.36255520e-02
   9.19727430e-02 -5.43249661e-01  3.70056144e-01  4.01674817e-02
  -6.51483319e-02]
 [ 2.52292002e-01  6.10591140e-01  6.90406760e-01 -2.64186838e-01
  -3.03078666e-01 -1.91999369e-01 -2.12781170e-01 -1.69928658e-01
   1.44277704e-01 -1.97563448e-01 -1.19228396e-01 -1.88654590e-01
   1.84267455e-01 -5.96292690e-02 -2.04637789e-01 -3.38680115e-01
   1.02364793e-01 -1.47856682e-01  3.30981645e-01  2.08050870e-01
   3.98360399e-01 -8.30421730e-02  6.24988682e-02  8.68609389e-01
   1.09281866e-01]
 [-2.47788746e-01  3.70070138e-01 -3.34004814e-01 -4.21935378e-01
   2.96466222e-01  2.39937274e-01 -2.70815638e-04  1.04952917e-01
  -3.67536574e-03 -2.37416918e-01 -1.43289893e-01 -4.37566356e-01
  -1.12944397e-01  3.31803325e-01  4.49727184e-01 -1.50301332e-01
   3.69074333e-02  1.24929953e-01  1.98066487e-01 -4.77028808e-01
  -1.11307383e-01  2.34989565e-01 -1.95102335e-01  1.55714523e-01
   3.67909907e-01]
 [ 5.34876375e-01  3.44327755e-01 -7.77907604e-02 -1.64139657e-01
  -2.59517661e-01  4.69602479e-01  2.95561003e-02 -1.47979639e-02
   5.27528831e-01  2.61489308e-01  9.15853534e-02 -3.00694581e-01
  -9.87266377e-02 -7.14824531e-02 -7.78706560e-01  2.24415174e-02
   2.75992400e-02 -9.33231796e-02  1.87156890e-01  5.65617565e-01
  -3.07816779e-01  1.09536903e-01 -2.12650028e-01 -6.62229802e-02
   1.62548290e-01]
 [ 2.28116883e-01 -3.66603531e-02 -3.43532210e-01  4.12125337e-01
   1.05976610e-01 -2.27858404e-01  5.43262915e-03 -2.45753461e-01
   4.59073316e-01 -2.26802058e-01 -3.35320238e-01  4.40176661e-01
  -1.14319117e-01  2.68684583e-01 -5.07136274e-01 -1.64132620e-01
   2.46354043e-01  2.13378447e-01  5.05835569e-01 -7.19067603e-02
  -1.45934988e-01 -4.58383961e-02  2.49284086e-02 -3.50753642e-01
   5.31122874e-01]
 [-4.58265262e-01  1.36566825e-01  2.88125129e-01 -3.80003719e-01
  -4.07841503e-01  1.50439339e-01 -1.51456964e-01  2.27797000e-01
   1.24253696e-01  3.63487727e-01  4.74357959e-01 -6.77189209e-01
  -3.64036268e-01 -4.18920833e-01  2.18987949e-01  5.90737132e-02
  -1.09744470e-01  5.19177427e-01  2.39626618e-01 -3.93790393e-01
   1.58910549e-01  4.53947778e-01  3.79662791e-01  3.59315649e-01
   1.86232991e-01]
 [ 8.62897256e-01  9.41643634e-01 -1.88676687e-01 -4.72650208e-02
   3.05775045e-01  1.04845781e-01  7.98659632e-01  1.91669980e-01
  -5.67741767e-02 -3.59648601e-01 -1.64264685e-01  8.68734720e-01
   3.29465633e-01  3.14935778e-01 -1.89100586e-01 -1.68860379e-01
   2.40285616e-01  2.98602396e-01  6.27424155e-01  1.12926736e-01
  -4.24622899e-02  1.15527415e-01  1.07997906e-01  4.92168804e-01
  -1.75007229e-01]
 [-3.75234041e-01  5.47634593e-02 -1.48189566e-01  7.90024052e-02
  -5.02307924e-01 -5.01270153e-01  3.75249912e-02  9.92280436e-03
   2.96541491e-01 -5.59301028e-01  1.61156804e-01  4.56947236e-01
   2.21449352e-01  3.58263834e-01 -8.42026840e-01  1.78608892e-01
  -2.61199674e-01 -3.35837846e-01 -1.76581803e-01 -1.79879104e-01
   2.44081339e-01 -6.25761867e-01 -1.16713110e-01 -2.66035847e-01
  -2.37603388e-01]
 [-4.99817927e-01  9.35711046e-02 -5.55229609e-01 -7.01898903e-02
   1.48133430e-01 -1.89740911e-01 -3.71210242e-02 -1.56194750e-01
  -2.45339233e-01 -1.20383556e-01  2.03664709e-01 -9.24448402e-02
   2.87563382e-01 -1.97479501e-01 -6.10122739e-02 -4.46977659e-01
  -1.16874095e-01  2.84050354e-01  2.33514525e-01 -2.58891406e-01
  -3.62382290e-02  1.28138563e-01  7.25753905e-02  3.94322283e-01
   8.48032698e-01]
 [-2.35120829e-02  8.08432650e-01 -2.02381872e-01 -3.16522694e-01
  -7.18126443e-02 -1.20126019e-03  6.16240385e-02  1.67020660e-01
  -1.21673269e-01 -2.68391491e-01  1.37703180e-01 -1.63989852e-02
  -2.00895257e-02  2.36297552e-01 -4.53876964e-01  4.65867217e-01
   4.20948014e-01  3.08946494e-01 -4.58031514e-01 -2.71272961e-01
   3.41870263e-01  2.18613275e-02 -6.64927612e-04  3.31674366e-02
  -2.21684791e-01]
 [-1.17813557e-02 -2.81343428e-01 -7.60902840e-02  4.23901764e-02
   2.41549182e-01  1.06088958e-01 -8.15356444e-01 -2.63905116e-01
  -2.60959182e-02 -4.56988059e-01  2.02643681e-01 -5.18905913e-01
  -6.91896096e-01  2.54118325e-01  3.34523094e-02 -5.42044225e-02
  -7.54152649e-02 -2.64906395e-01 -2.68282850e-01 -1.39399829e-01
   7.01276746e-02  3.27131295e-01 -5.93539531e-01  3.74364561e-01
   3.93540690e-02]
 [ 2.80166995e-01  1.69546804e-01  3.14404097e-01  1.23520606e-01
  -5.38593792e-01  2.47344522e-01  4.23046435e-01 -2.20543870e-01
   1.18102540e-01  5.48660647e-01 -3.90478970e-01  6.75454906e-01
  -2.44165683e-01 -3.22132697e-01  7.66346190e-02 -1.06173694e-01
  -8.17937023e-01  5.45585247e-01 -2.14360213e-01 -4.65486962e-01
  -7.41321434e-01  9.90289682e-02  5.25944940e-01  3.96114579e-01
  -1.02636926e-01]] 
Matrix V:
 [[ 0.15946759  0.1275968  -0.52485812 ... -0.22461176  0.58171411
  -0.21453343]
 [-0.07557585  0.10257819 -0.1289723  ...  0.03701577 -0.06111926
   0.65056264]
 [-0.0653268   0.59152168 -0.48971421 ...  0.06186635 -0.14627528
  -0.55860167]
 ...
 [ 0.03574904  0.13913748  0.0445319  ...  0.60054254  0.37258521
  -0.01741619]
 [-0.2440009  -0.23961543 -0.07371553 ... -0.12161262 -0.39708757
   0.38019622]
 [ 0.01424396 -0.0006716  -0.0665955  ... -0.24988688  0.33035464
   0.39563031]] 
Matrix W:
 [[-0.393734    0.18560492  0.44453796 ... -0.3780781   0.25664736
   0.76134898]
 [ 0.09361634  0.13057611 -0.5610579  ... -0.08607488  0.29398292
  -0.46731163]
 [ 1.35142891  0.31024749  0.30002135 ...  0.18019355  0.55230674
   0.67805078]
 ...
 [-0.19243786  0.28829924 -0.09711331 ... -0.04097314 -0.1169756
  -0.33220634]
 [ 0.68560422  0.24036963  0.21992336 ... -0.31170154  0.46702782
  -0.0336354 ]
 [-0.47557761  0.00373244  0.25808705 ...  0.11337031  0.30283273
  -0.15598887]] 
===========================


Training model for 10 epochs
training set: 1000 sentences (batch size 100)
Optimizing loss on 1000 sentences
Vocab size: 2000
Hidden units: 25
Steps for back propagation: 5
Initial learning rate set to 0.05, annealing set to 5

calculating initial mean loss on dev set: 7.975848348526116

epoch 1, learning rate 0.0500	instance 1	epoch done in 49.68 seconds	new loss: 6.2362781424192955
epoch 2, learning rate 0.0417	instance 1	epoch done in 49.30 seconds	new loss: 5.7821832680719165
epoch 3, learning rate 0.0357	instance 1	epoch done in 49.95 seconds	new loss: 5.620869982950247
epoch 4, learning rate 0.0312	instance 1	epoch done in 49.61 seconds	new loss: 5.542282337857089
epoch 5, learning rate 0.0278	instance 1	epoch done in 49.56 seconds	new loss: 5.494356929324897
epoch 6, learning rate 0.0250	instance 1	epoch done in 49.36 seconds	new loss: 5.458775499913881
epoch 7, learning rate 0.0227	instance 1	epoch done in 49.17 seconds	new loss: 5.431707747538838
epoch 8, learning rate 0.0208	instance 1	epoch done in 49.31 seconds	new loss: 5.408627400349114
epoch 9, learning rate 0.0192	instance 1	epoch done in 49.09 seconds	new loss: 5.389825700069598
epoch 10, learning rate 0.0179	instance 1	epoch done in 49.02 seconds	new loss: 5.373065147917203

training finished after reaching maximum of 10 epochs
best observed loss was 5.373065147917203, at epoch 10
setting U, V, W to matrices from best epoch

==========================
Matrix U:
 [[ 6.26316261e-01  1.94703752e-01  4.63603247e-01  1.37527203e-01
  -5.19852570e-03 -1.86410380e-01  2.26383247e-01 -9.95712796e-02
  -3.60401536e-01  8.64973177e-02  1.58972944e-01 -4.65918461e-02
   1.65710870e-01  1.82326291e-01 -3.39943512e-01 -1.43409952e-01
  -1.37409436e-01  1.45117422e-01 -7.42266461e-01 -3.59688782e-01
  -2.86681536e-01 -7.13271780e-01  3.30392889e-02 -2.66105044e-01
   7.52683319e-01]
 [-1.37025317e-01  4.30612493e-01  2.53325815e-01 -2.67162504e-01
   1.33123999e-01  1.09006364e-01 -8.48978061e-02  7.88144098e-02
  -4.71607131e-02 -5.49059404e-01 -3.92912206e-01  2.15925400e-01
  -7.49314443e-02 -3.40317648e-01 -9.53369182e-03  4.17600666e-01
   2.39133415e-01  2.88527050e-01 -2.30006678e-01  2.97433926e-01
  -4.88696316e-01 -5.34824681e-01  1.25884281e-01 -9.42247454e-02
  -2.78447694e-01]
 [-2.88856566e-02 -4.78505445e-01  4.71359593e-01  3.75544061e-01
  -6.03565751e-01 -2.56472195e-01 -5.50948888e-02  3.72829115e-01
  -2.68163786e-01  4.06915092e-01  8.36868810e-02 -7.01181643e-01
  -6.30671280e-01 -5.79892301e-01  1.81182132e-01 -5.67029810e-02
   4.95498690e-01  1.61316678e-01  3.51192856e-01  1.70142886e-01
  -4.86961566e-01 -8.00574478e-02 -7.18707613e-01 -5.04903593e-02
   1.79747091e-01]
 [-2.72815213e-01  1.66966947e-01  2.47831837e-01 -1.67423105e-01
  -1.83215063e-01 -2.86337099e-01 -1.49166840e-01 -4.95535259e-03
  -3.47272261e-01 -1.33417619e-01  3.19850523e-01 -2.55726590e-01
  -1.40890273e-02  2.61378252e-01  9.51238759e-02 -2.76882279e-01
  -3.94883998e-01 -2.22656364e-01  3.51419567e-02 -8.93416511e-04
  -4.86128632e-01  1.22295402e-01 -2.04269974e-01 -3.16778123e-01
   2.73047227e-02]
 [-1.24176792e-01  6.39212785e-02  7.02702678e-04  1.36466260e-01
  -1.25036792e-01  7.71302200e-02  5.00551050e-01  6.37198267e-02
  -1.68950764e-01 -3.97510514e-01  1.02103926e-01  5.56709234e-02
  -8.10882188e-02  4.66743258e-01  2.11999861e-01 -5.26643823e-01
  -4.81087664e-01  1.97098575e-01  1.07642824e-02  5.43282773e-01
  -1.10298849e-01 -3.27469730e-01  1.46368984e-01 -3.33607720e-02
  -2.31885373e-01]
 [ 4.47419595e-01 -1.49928270e-01  1.51676669e-01  3.30875074e-02
   6.69475202e-01  3.50914394e-01  2.26233264e-01  3.48615901e-01
   4.81148874e-01  3.80189479e-02 -7.65465371e-02 -3.17860579e-01
   1.96848133e-01 -1.20346309e-01  1.60350961e-01  2.69444763e-02
  -3.27947808e-01 -5.86916726e-02 -4.27793625e-01  2.79606568e-01
   2.46965323e-02 -4.67676258e-01  3.46907350e-01  2.21647071e-01
   1.02426895e-01]
 [ 2.88835531e-01  4.88762508e-02 -4.12832441e-01  2.81454369e-01
  -1.68108527e-01  2.38784454e-01  6.12640306e-01 -5.36664448e-01
   3.93041772e-01 -3.44298844e-01  2.56060735e-01  2.59845992e-02
  -4.70728837e-01  7.42448933e-02  2.87555447e-01  2.12367099e-01
   5.91300654e-01 -4.82939720e-02 -5.80666678e-02 -7.73497958e-02
  -4.17258640e-03  2.54684770e-01 -1.08940904e-01  1.69147774e-01
  -1.03593032e-01]
 [ 1.39824487e-01 -1.49706856e-01  1.23735278e-01  4.87203403e-01
   7.39846616e-01  4.41267797e-01 -1.29172683e-01  2.51264987e-01
   4.89295094e-01 -1.86233700e-01  7.95203047e-02 -4.78723690e-02
  -5.29074820e-01  2.01567377e-02  1.87590851e-01 -1.61463411e-02
   3.64549010e-01 -3.33319315e-02  1.60352726e-01  4.74694842e-01
   5.18621871e-01  1.87193798e-01 -2.42538709e-01 -3.28896658e-01
   7.35695082e-02]
 [-5.43213246e-02  3.04454816e-01  2.52365390e-01 -8.67428812e-02
   3.60367001e-01  1.10854944e-01  1.58855602e-01  2.50453784e-01
  -6.59794381e-02  3.76468194e-01  1.97974498e-01 -7.28237073e-02
   7.59401364e-01  2.16915316e-01 -1.79542166e-01  1.11710624e-01
  -2.46030437e-01  2.36615438e-01  1.41097165e-01 -3.03412917e-01
   4.05516364e-01  1.84796130e-01  7.02248531e-01  1.11052057e+00
  -2.00254566e-01]
 [ 2.55431168e-01 -3.12722988e-01  3.40297202e-01  3.84172779e-02
  -6.51466034e-01 -1.45064294e-01  4.02820493e-01  5.49672970e-01
  -3.07627895e-01  5.44100414e-01 -1.93193088e-01  2.35280940e-01
   1.93445175e-01  3.82269787e-03  2.79147172e-01  1.03382677e-01
  -4.98272889e-01 -1.70967865e-01 -7.72590386e-02  2.32291408e-01
   3.62549733e-01  4.19569811e-01  1.86576134e-01 -3.78840637e-01
  -4.03248652e-01]
 [-1.37519610e-01  2.36668358e-01 -1.75132841e-01 -4.57128696e-01
  -2.43390088e-01 -7.86388878e-02  9.55763396e-02  1.75419060e-01
   2.29141857e-01  2.36906255e-01 -8.85860002e-01 -4.97460587e-02
  -1.20631883e-01  1.87834869e-03 -5.69358158e-01  5.46667374e-01
   9.78313659e-02 -2.57950507e-01 -3.32789229e-01  3.29460290e-01
  -3.88196138e-01  1.39521779e-01  4.87036938e-01 -3.82858929e-02
   1.02458716e-01]
 [-5.56531881e-02 -3.46548923e-01  2.99315856e-01 -4.58502887e-01
  -2.57737994e-01 -8.97482322e-01 -1.42334740e-01 -4.85321146e-01
  -1.31467035e-01 -1.62708984e-03 -3.11054042e-02  3.18729761e-01
  -2.29296232e-01  8.01203014e-03 -4.23520950e-01  7.99048338e-02
   1.01203309e-01  3.33781483e-01 -3.48187349e-01 -3.69765967e-01
  -2.04863834e-01  1.99674135e-01  1.99931275e-01  2.48324638e-02
  -1.36954973e-01]
 [ 1.96751136e-01  2.80791014e-01 -9.71516177e-06 -3.59468168e-01
  -1.31168227e-01  1.89084039e-01 -5.98004703e-01  7.57655042e-02
  -1.43847198e-01 -2.10377376e-01  1.01068640e-01 -8.86020147e-02
   4.11421933e-01 -4.72593727e-01  2.91529266e-01 -2.50037312e-01
  -2.10423334e-02  1.81526028e-01 -2.90253833e-01 -5.42300712e-01
  -3.90826985e-01  1.51862951e-01 -1.05909084e-01 -1.64367381e-01
  -1.24137184e-01]
 [-1.11541608e-01  2.42816608e-01  1.01521075e-01  3.61227179e-01
   3.16678076e-02 -3.23885305e-01 -5.93279072e-01 -2.00960846e-01
  -7.86178787e-01  7.96705109e-02  1.26380163e-02  5.16532711e-01
  -1.12918722e-01 -4.36194306e-01 -1.23436525e-01 -1.83269106e-01
  -1.74989690e-01 -5.52280475e-01  8.06161923e-03  5.24373508e-01
   2.27791700e-01  3.13804137e-01 -5.86455006e-01 -2.62816183e-01
   3.20262500e-01]
 [ 4.74075400e-01  3.02271113e-02 -4.79605257e-01 -1.01366850e-01
  -9.74472111e-03 -4.17967484e-01 -1.88848147e-01 -2.07010067e-01
  -4.65906474e-02  2.20420396e-01  2.70895533e-02 -4.85345108e-02
  -1.82858163e-01 -5.06587957e-01  2.28212293e-01  2.48784783e-02
  -2.20816604e-01  2.80542625e-01 -1.07972552e-01 -4.63799081e-01
  -6.30402067e-03 -1.31136050e-01 -1.76585423e-01 -1.17913085e-01
   3.35189766e-01]
 [ 7.62361071e-01 -2.85898539e-01  1.44220714e-01  2.93932669e-01
   3.20786606e-01 -3.79527474e-01  6.08352376e-01  7.88799801e-01
   5.44628191e-01  2.62303419e-01  7.53523342e-02 -2.35415936e-01
  -7.25177727e-02  2.56604398e-01  1.46103371e-02 -5.75186859e-02
   2.64364538e-01  3.02259513e-02  1.98921762e-01  3.25569169e-01
   3.41840000e-01 -3.24233313e-01  1.23585444e-01  3.41217438e-01
  -3.12965329e-01]
 [ 6.81310898e-02  3.86670381e-01  8.47258690e-02  6.04826417e-01
   8.04988827e-02 -2.16234007e-02 -5.80970506e-01  3.10048316e-01
   1.25581516e-01  5.42878424e-01 -2.58824846e-03 -4.31519630e-01
   4.12640346e-01  7.13143998e-02 -2.85807552e-01  8.20262806e-02
   5.14033928e-01  2.54607770e-01  4.69124108e-02  6.39366171e-01
   5.43827105e-01  1.58287957e-01  3.10193335e-01 -4.53806123e-01
  -1.04708727e-01]
 [-4.01603099e-01 -4.69106917e-01 -4.89862823e-02  8.33829910e-02
  -8.29352143e-02 -8.88568496e-02 -1.18279728e-01  4.62546372e-01
   1.36425023e-01  1.80896661e-01 -2.07964530e-02  9.25284113e-03
  -8.22331893e-02  3.89615657e-01  7.40231980e-01  1.80079611e-01
  -1.15224827e-01  5.54831051e-01 -2.04016137e-01  2.29827708e-02
   3.50112085e-02 -3.54274765e-01  2.78161017e-01  1.83786462e-01
   2.70033974e-01]
 [-1.04250961e-01 -3.11139387e-01 -9.14499198e-02  1.11265960e-01
  -3.19220503e-01 -6.06044195e-02 -5.08023693e-01 -3.40093113e-01
   2.13479392e-01 -1.06474536e-01 -1.92945180e-01  6.56439619e-02
  -2.10898829e-01 -2.99994969e-01  3.42592268e-01  1.78017868e-01
  -3.46931573e-01 -2.24777418e-01 -3.80188078e-01 -2.87092421e-01
   4.05229443e-01 -4.07398687e-01 -8.54259340e-02  2.68370339e-01
   9.97594634e-02]
 [ 1.91597517e-01 -8.64303605e-02  5.79915037e-01 -1.50498888e-01
   1.35514554e-01 -2.47654521e-01 -1.40444101e-01  1.14047321e-01
   2.60723369e-01 -1.37882869e-01  3.71470465e-02  4.26552329e-02
   9.29705205e-02 -4.32555225e-02 -1.51995619e-01  1.96508189e-01
   5.48186774e-01  3.92289842e-01 -6.48588183e-01 -2.20866692e-01
   5.54742052e-02 -4.11411302e-01 -5.90704957e-02  4.60013237e-02
   5.77884194e-01]
 [ 2.80505403e-02 -8.00288369e-02  1.36642634e-01  2.78932582e-03
   1.62728334e-01  9.19276475e-02  3.75993109e-01 -4.61363720e-01
  -2.05279173e-02  1.02005714e-01  4.36104601e-01 -1.20428876e-01
   3.88631966e-01 -9.33370958e-03 -3.91643454e-01  2.08689631e-01
  -1.64734012e-01  3.34755986e-01 -4.63599624e-01  4.65371993e-01
  -3.43528051e-01 -2.08483609e-01  2.77454264e-01  7.45351961e-01
  -2.26766528e-01]
 [ 7.23390765e-02 -3.39030191e-01 -1.70742313e-01 -1.22278540e-01
  -2.28330327e-01 -4.33830346e-01  2.89529070e-01 -4.24319601e-01
  -5.06384674e-01  3.35177345e-01 -3.86033952e-02  1.73182605e-01
   1.76753431e-01  3.97925391e-02  6.67394059e-02 -1.57398158e-01
  -4.18557854e-01 -4.39338803e-01 -2.40845046e-01  2.23672075e-01
   3.15001872e-01  2.20091888e-02 -1.22671726e-01  1.98048216e-01
  -1.05330401e-02]
 [-2.97498650e-01 -1.74598096e-01 -5.29111822e-01 -1.32463415e-01
   3.87009127e-01  1.63737760e-01  2.75242999e-01  1.91328025e-01
   1.31928005e-01 -8.74066140e-04  2.39662106e-01 -1.03797988e-01
   3.64598090e-01  4.95146864e-02 -3.02773960e-01 -2.98087687e-02
   1.99986077e-01 -4.01395277e-01 -2.57325038e-01 -4.14039354e-01
  -5.14165279e-02  1.71296028e-01 -8.47375441e-01 -4.86134101e-01
  -5.70972543e-02]
 [-4.60920320e-01  1.21182092e-01 -4.77380234e-02  3.99533012e-01
   2.49595850e-01 -7.04800482e-02  9.38913428e-02  1.27756618e-02
  -1.85722553e-01  5.75778344e-01 -2.97017900e-02  5.72183199e-02
  -1.98437539e-01  2.00495313e-01 -4.96816277e-01  8.61921333e-02
  -1.21412368e-01 -4.23619653e-01 -4.87701679e-02  3.24212040e-02
   4.74727742e-02 -1.95291993e-01 -4.08994693e-01 -4.68167282e-03
   4.69895270e-01]
 [ 1.82120848e-02 -7.92676671e-01  1.40394168e-01 -1.79480522e-01
   2.69794276e-01 -7.57621010e-03  1.18853833e-01 -1.27610542e-01
   1.43255916e-01 -4.56699148e-01 -1.04822217e-01  3.35114104e-02
   6.53864676e-01 -9.75085309e-02  2.88300351e-01  1.89439536e-01
   3.23377001e-02  2.77016712e-01 -1.35265091e-01  3.80187007e-01
   4.23719906e-01 -5.74644567e-02  4.54364546e-02  8.82039161e-02
   2.58983427e-01]] 
Matrix V:
 [[-0.08319239  0.47131451  0.07573486 ...  0.04116172  0.40720675
  -0.2125099 ]
 [-0.36239533  0.0910683   0.00911855 ... -0.07994197 -0.636189
   0.38082394]
 [ 0.59949358  0.57818501  0.30971168 ...  0.91310056  0.22663397
   0.43431924]
 ...
 [ 0.09814118 -0.18810768  0.35447934 ... -0.33668333  0.44634708
  -0.23584295]
 [ 0.20032084  0.22115829  0.18780794 ... -0.00939275 -0.17217401
   0.23137385]
 [-0.23663056  0.01505964  0.11269329 ... -0.31178596 -0.49705552
   0.0561939 ]] 
Matrix W:
 [[-0.12914731 -0.20169411  0.08624801 ...  0.34345603  0.80740258
   0.47534634]
 [-0.44181805  0.01792109 -0.11065318 ... -0.42605647 -0.30727447
   0.33316103]
 [ 0.80521166  0.42577925  0.65808617 ...  0.23397654  0.73975501
   0.35992746]
 ...
 [ 0.14117975 -0.24796391  0.52653183 ...  0.13695677  0.46155973
   0.47912406]
 [-0.53212765  0.438575   -0.00859084 ... -0.1620645  -0.53014612
  -0.13350648]
 [ 0.00255233 -0.10383421 -0.07000834 ...  0.54575448  0.11216323
  -0.26751583]] 
===========================


Training model for 10 epochs
training set: 1000 sentences (batch size 100)
Optimizing loss on 1000 sentences
Vocab size: 2000
Hidden units: 50
Steps for back propagation: 0
Initial learning rate set to 0.05, annealing set to 5

calculating initial mean loss on dev set: 7.9278225747898095

epoch 1, learning rate 0.0500	instance 1	epoch done in 40.85 seconds	new loss: 5.94090205535138
epoch 2, learning rate 0.0417	instance 1	epoch done in 40.45 seconds	new loss: 5.602166467059012
epoch 3, learning rate 0.0357	instance 1	epoch done in 40.71 seconds	new loss: 5.486052139394797
epoch 4, learning rate 0.0312	instance 1	epoch done in 40.52 seconds	new loss: 5.424441192651991
epoch 5, learning rate 0.0278	instance 1	epoch done in 40.81 seconds	new loss: 5.381836157626984
epoch 6, learning rate 0.0250	instance 1	epoch done in 40.42 seconds	new loss: 5.349032180742898
epoch 7, learning rate 0.0227	instance 1	epoch done in 40.12 seconds	new loss: 5.322961468400411
epoch 8, learning rate 0.0208	instance 1	epoch done in 41.14 seconds	new loss: 5.301185899650594
epoch 9, learning rate 0.0192	instance 1	epoch done in 40.23 seconds	new loss: 5.282966962502312
epoch 10, learning rate 0.0179	instance 1	epoch done in 40.43 seconds	new loss: 5.267207273043813

training finished after reaching maximum of 10 epochs
best observed loss was 5.267207273043813, at epoch 10
setting U, V, W to matrices from best epoch

==========================
Matrix U:
 [[ 1.67445761e-01  4.33939148e-01 -5.02379119e-01 ... -1.73905188e-01
   6.08435464e-01  1.64466204e-01]
 [ 1.79436345e-01  1.86615582e-01  1.97574328e-01 ... -2.67568299e-01
  -2.35050198e-01 -3.51546612e-02]
 [-5.12418547e-01  1.09393214e-01 -5.92684243e-01 ... -2.70435660e-01
   2.85794200e-02 -9.29246752e-01]
 ...
 [ 4.66582152e-01 -2.62225831e-01 -1.95441523e-02 ... -3.44040105e-02
   3.91390451e-02  3.62256256e-04]
 [-1.38810131e-01  1.31743114e-01 -2.74591596e-01 ... -2.19693034e-01
   2.61609005e-01 -2.13402874e-03]
 [ 4.72632938e-01 -4.86498180e-01  6.99510472e-01 ... -2.82972999e-01
  -3.41835209e-01 -1.58453357e-02]] 
Matrix V:
 [[-0.01739863 -0.20333003 -0.01280963 ... -0.07780955  0.26491491
  -0.54507178]
 [-0.05140401 -0.33262983 -0.49494171 ... -0.68422148  0.11142317
  -0.28924117]
 [ 0.03390834  0.20433785 -0.02777217 ...  0.47438631  0.02812045
   0.04350255]
 ...
 [ 0.02966116  0.04537504  0.07659012 ...  0.32553667 -0.39141443
   0.15536376]
 [-0.04105467 -0.15390415 -0.52629425 ...  0.02937574  0.17754274
   0.42747088]
 [-0.30648556  0.24667524  0.23046085 ...  0.78021244 -0.13947167
  -0.06912205]] 
Matrix W:
 [[-0.05905737  0.51199255 -0.07524814 ...  0.34287624  0.21268412
   0.9029283 ]
 [-0.17141433  0.05722252  0.00279044 ...  0.32570222 -0.12960702
  -0.45359576]
 [ 0.18271956 -0.13729504  0.25922907 ...  0.03872888  0.1987711
   0.40344852]
 ...
 [-0.24243839  0.15707266  1.20339922 ...  0.15994583  0.08169408
  -0.28025785]
 [-0.01767071 -0.55510538  0.06420647 ... -0.4032166  -0.44845515
   0.01423921]
 [ 0.23833876 -0.18299246 -0.18842459 ... -0.1991602   0.04538696
  -0.62910709]] 
===========================


Training model for 10 epochs
training set: 1000 sentences (batch size 100)
Optimizing loss on 1000 sentences
Vocab size: 2000
Hidden units: 50
Steps for back propagation: 2
Initial learning rate set to 0.05, annealing set to 5

calculating initial mean loss on dev set: 8.497008344064579

epoch 1, learning rate 0.0500	instance 1	epoch done in 50.84 seconds	new loss: 5.864541512364173
epoch 2, learning rate 0.0417	instance 1	epoch done in 51.16 seconds	new loss: 5.60202638369902
epoch 3, learning rate 0.0357	instance 1	epoch done in 50.57 seconds	new loss: 5.504592362063118
epoch 4, learning rate 0.0312	instance 1	epoch done in 51.23 seconds	new loss: 5.444712114208767
epoch 5, learning rate 0.0278	instance 1	epoch done in 50.43 seconds	new loss: 5.402287558827438
epoch 6, learning rate 0.0250	instance 1	epoch done in 51.17 seconds	new loss: 5.370080691739339
epoch 7, learning rate 0.0227	instance 1	epoch done in 50.30 seconds	new loss: 5.343596523670612
epoch 8, learning rate 0.0208	instance 1	epoch done in 50.26 seconds	new loss: 5.320978322949617
epoch 9, learning rate 0.0192	instance 1	epoch done in 50.04 seconds	new loss: 5.302052164056739
epoch 10, learning rate 0.0179	instance 1	epoch done in 50.06 seconds	new loss: 5.285417489615456

training finished after reaching maximum of 10 epochs
best observed loss was 5.285417489615456, at epoch 10
setting U, V, W to matrices from best epoch

==========================
Matrix U:
 [[ 0.77869125 -0.12621352 -0.27623404 ...  0.20717803 -0.57003397
  -0.14506964]
 [-0.53951984 -0.11461886 -0.18135183 ... -0.10632288 -0.26890218
  -0.29865451]
 [ 0.15603264 -0.2040427   0.3347458  ... -0.44726556  0.04849834
  -0.7008762 ]
 ...
 [-0.18425835  0.08871005 -0.14483072 ...  0.24586331 -0.77551954
  -0.16818721]
 [-0.2910355   0.15725167  0.13154197 ...  0.57231254  0.03018065
  -0.27165128]
 [-0.26109009  0.1802902  -0.00257962 ... -0.27576595 -0.26161629
  -0.13997336]] 
Matrix V:
 [[-0.19527116 -0.27307726 -0.08448185 ...  0.25478825  0.24673484
   0.02274931]
 [ 0.53039645  0.21065903  0.19466869 ...  0.16742754  0.16188961
   0.03664414]
 [-0.65423247 -0.42835772  0.60991909 ...  0.13621616  0.10322926
  -0.26370592]
 ...
 [-0.15493075  0.32575611  0.51226741 ...  0.34664135 -0.67687165
   0.29570936]
 [ 0.05558001 -0.30561272 -0.04238103 ... -0.40350303 -0.11914135
  -0.32991465]
 [ 0.53597702 -0.08120985  0.36351622 ... -0.15038746  0.98015088
   0.34016577]] 
Matrix W:
 [[ 0.33216568 -0.31080391  0.62224371 ...  0.38445908  0.23888445
  -0.00313573]
 [ 0.18248803 -0.03838841 -0.45156898 ... -0.09018247  0.12210379
  -0.17715247]
 [ 0.49920955  1.13429828  0.44895562 ...  0.49727765  0.43079341
   0.22697156]
 ...
 [-0.13001201 -0.48080294  0.21471215 ... -0.11757871 -0.09758897
   0.27821695]
 [ 0.21967832  0.0496863   0.02516345 ... -0.236158    0.03707468
   0.10430358]
 [-0.24404825  0.00339389  0.09852219 ... -0.49312847 -0.10121339
  -0.0018358 ]] 
===========================


Training model for 10 epochs
training set: 1000 sentences (batch size 100)
Optimizing loss on 1000 sentences
Vocab size: 2000
Hidden units: 50
Steps for back propagation: 5
Initial learning rate set to 0.05, annealing set to 5

calculating initial mean loss on dev set: 8.07173846999181

epoch 1, learning rate 0.0500	instance 1	epoch done in 64.73 seconds	new loss: 5.975188832975169
epoch 2, learning rate 0.0417	instance 1	epoch done in 97.02 seconds	new loss: 5.664230372082029
epoch 3, learning rate 0.0357	instance 1	epoch done in 66.63 seconds	new loss: 5.553720599103528
epoch 4, learning rate 0.0312	instance 1	epoch done in 64.44 seconds	new loss: 5.488633801103792
epoch 5, learning rate 0.0278	instance 1	epoch done in 65.70 seconds	new loss: 5.443761042115766
epoch 6, learning rate 0.0250	instance 1	epoch done in 64.87 seconds	new loss: 5.407800787808108
epoch 7, learning rate 0.0227	instance 1	epoch done in 64.72 seconds	new loss: 5.378914385209316
epoch 8, learning rate 0.0208	instance 1	epoch done in 64.89 seconds	new loss: 5.354977374276236
epoch 9, learning rate 0.0192	instance 1	epoch done in 64.79 seconds	new loss: 5.334003811396481
epoch 10, learning rate 0.0179	instance 1	epoch done in 64.95 seconds	new loss: 5.31625329379674

training finished after reaching maximum of 10 epochs
best observed loss was 5.31625329379674, at epoch 10
setting U, V, W to matrices from best epoch

==========================
Matrix U:
 [[ 0.02837058 -0.33486678  0.01609233 ...  0.20143577 -0.25922019
  -0.03901697]
 [-0.02867639 -0.02238636 -0.60095715 ... -0.10976951  0.06442899
   0.39762667]
 [-0.12894356  0.0247457   0.26867709 ... -0.08366088  0.44763584
  -0.28651511]
 ...
 [ 0.23401222  0.43819252 -0.32505989 ... -0.23954077  0.20918492
  -0.0294215 ]
 [ 0.1174505  -0.15589987 -0.07768483 ...  0.50253321  0.02540701
  -0.64625196]
 [-0.10761617 -0.15826426 -0.06749571 ...  0.2484354  -0.64007539
   0.1579373 ]] 
Matrix V:
 [[-0.55027644  0.38420747 -0.06951208 ... -0.52181602 -0.37237036
   0.00333634]
 [ 0.27602741 -0.06059924  0.01473046 ... -0.04850999  0.20132047
   0.35677689]
 [ 0.36044535  0.18010651 -0.4695026  ... -0.41024376 -0.25934441
  -0.34466989]
 ...
 [-0.54638567  0.00233649  0.09282105 ...  0.21699662  0.00367307
  -0.46231535]
 [ 0.06638327  0.18405101  0.00683433 ...  0.16434127 -0.2625468
   0.45285029]
 [-0.28219807  0.27127546 -0.10623223 ...  0.13422449 -0.03709583
  -0.52819863]] 
Matrix W:
 [[ 0.51063151 -0.04667978  0.13717319 ...  0.63479822  0.27564375
  -0.30784747]
 [ 0.52974935 -0.10998145  0.11558903 ... -0.06240244 -0.48859216
   0.07156898]
 [ 0.11455427  0.40952881  0.10737792 ... -0.06005126 -0.01805671
   0.42766147]
 ...
 [ 0.40976509 -0.00690038  0.03549175 ...  0.2881467  -0.00958773
  -0.2608768 ]
 [ 0.25930792  0.26156742  0.11105995 ...  0.62504606 -0.15799684
   0.38055928]
 [-0.04848957 -0.05465531 -0.32143355 ... -0.09538483  0.22985031
  -0.3283261 ]] 
===========================

Unadjusted: 0.368
Adjusted for missing vocab: 0.368
